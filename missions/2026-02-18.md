# ðŸŽ¯ Launchpad PM Skills - 2026-02-18

**Today's hands-on projects:** 6 PM skills â€” beginner to expert
**Sources:** Launchpad Original
**Difficulty:** Beginner Â· Easy Â· Easy Â· Intermediate Â· Advanced Â· Expert

---

## Mission 1: Scrape and Compare Competitor Pricing Pages

**â±ï¸ Time:** 20â€“30 minutes
**ðŸ“Š Difficulty:** Beginner
**ðŸ› ï¸ Tools:** Claude Code, Python, requests, BeautifulSoup

### ðŸ’¡ What You're Building

A script that fetches your competitors' pricing pages and outputs a clean, structured markdown report you can paste into a doc or share in Slack â€” no manual copy-paste required.

**You'll have:**
- Python script that fetches any list of pricing page URLs
- Auto-extraction of pricing tiers, prices, and feature bullets
- Clean `competitive-pricing-YYYY-MM-DD.md` report saved to disk
- Reusable tool you can re-run weekly in 5 seconds

### âœ… Prerequisites

- Python 3.8+ installed (`python3 --version` to check)
- 3 competitor pricing page URLs ready to go

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Create the Project

Set up a clean workspace for your pricing intelligence tool

```bash
mkdir -p ~/pm-tools/pricing-tracker
cd ~/pm-tools/pricing-tracker
python3 -m venv .venv
source .venv/bin/activate
pip install requests beautifulsoup4 markdownify
touch scraper.py config.json
```

**Success Checklist:**
- [ ] Virtual environment activated (you see `.venv` in prompt)
- [ ] `pip install` completed without errors
- [ ] Both files created

#### Step 2: Configure Your Competitors

Create a config file with the URLs you want to track

```bash
cat > config.json << 'EOF'
{
  "competitors": [
    {"name": "Competitor A", "url": "https://example.com/pricing"},
    {"name": "Competitor B", "url": "https://example2.com/pricing"},
    {"name": "Competitor C", "url": "https://example3.com/pricing"}
  ],
  "output_dir": "reports"
}
EOF
mkdir -p reports
```

Replace the example URLs with your actual competitor pricing pages. Ask Claude Code: "Update config.json with these 3 competitor pricing URLs: [paste your URLs]"

**Success Checklist:**
- [ ] config.json has real competitor URLs
- [ ] reports/ directory created

#### Step 3: Build the Scraper

Ask Claude Code to write the scraping logic

```bash
# Tell Claude Code:
# "Write a scraper.py that:
#  1. Reads competitors from config.json
#  2. Fetches each pricing page with requests (5s timeout, browser User-Agent header)
#  3. Uses BeautifulSoup to extract: all price amounts ($X/mo patterns),
#     plan/tier names (h2, h3 near prices), and feature bullet lists
#  4. Saves a markdown report to reports/competitive-pricing-{today}.md
#     with one section per competitor showing their tiers, prices, and top features
#  5. Prints a summary to terminal when done"

# After Claude writes it, run:
python3 scraper.py
```

**Success Checklist:**
- [ ] Script runs without import errors
- [ ] At least one competitor's page fetched successfully
- [ ] Report file created in reports/

#### Step 4: Review and Refine the Report

Open the report and ask Claude to clean it up

```bash
# Open the report to see what was captured
cat reports/competitive-pricing-$(date +%Y-%m-%d).md

# Ask Claude Code:
# "The report has some noise â€” clean up the markdown, remove navigation text,
#  and add a summary table at the top comparing the pricing tiers side by side"
```

**Success Checklist:**
- [ ] Report is readable and clean
- [ ] Summary comparison table at top
- [ ] Ready to share with your team

### ðŸŽ¯ Success Criteria

- [ ] Script fetches all 3 competitor pages without crashing
- [ ] Report includes pricing tiers and feature lists for each competitor
- [ ] You can re-run it anytime with `python3 scraper.py` and get a fresh report
- [ ] Report is clean enough to paste into a Confluence page or Slack

### ðŸ° Next Steps (Optional)

Once you've got the basics working, try:

- Add a cron job to run this weekly: `crontab -e` â†’ `0 9 * * 1 cd ~/pm-tools/pricing-tracker && source .venv/bin/activate && python3 scraper.py`
- Track changes over time by diffing reports: ask Claude Code to add a "What changed since last week" section
- Add your own product's pricing to the report for side-by-side comparison

---

## Mission 2: Build an App Store Review Theme Extractor

**â±ï¸ Time:** 25â€“40 minutes
**ðŸ“Š Difficulty:** Easy
**ðŸ› ï¸ Tools:** Claude Code, Python, app-store-scraper

### ðŸ’¡ What You're Building

A tool that pulls your last 200 App Store reviews, clusters them into themes, and produces a ranked insight report with representative quotes â€” the kind of analysis that normally takes a PM 2 hours of manual reading.

**You'll have:**
- Script that fetches reviews for any iOS app by App Store ID
- Automatic theme clustering (pain points, praise, feature requests)
- Ranked output showing which themes appear most
- A `review-analysis-YYYY-MM-DD.md` you can bring to your next product review

### âœ… Prerequisites

- Python 3.8+ installed
- Your app's App Store ID (find it in the App Store URL: `apps.apple.com/app/id{YOUR_ID}`)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

```bash
mkdir -p ~/pm-tools/review-analyzer
cd ~/pm-tools/review-analyzer
python3 -m venv .venv
source .venv/bin/activate
pip install app-store-scraper anthropic python-dotenv
touch analyzer.py .env
```

**Success Checklist:**
- [ ] All packages installed
- [ ] .env file created (will hold your API key)

#### Step 2: Add Your API Key

```bash
echo "ANTHROPIC_API_KEY=your_key_here" >> .env
# Replace 'your_key_here' with your actual Anthropic API key
# (This powers the theme clustering â€” the scraping is free)
```

**Success Checklist:**
- [ ] .env has real ANTHROPIC_API_KEY

#### Step 3: Fetch Reviews

Ask Claude Code to write the review fetcher

```bash
# Tell Claude Code:
# "Write a Python script analyzer.py with:
#   1. fetch_reviews(app_id, country='us', count=200) using app_store_scraper
#      Returns a list of dicts: {rating, title, review, date}
#   2. Save raw reviews to raw-reviews.json so we can rerun analysis
#      without re-fetching
#   3. Print how many reviews were fetched and average rating"

# Run it with your app ID (example: 284882215 = Facebook):
APP_ID=284882215  # replace with your app ID
python3 -c "from analyzer import fetch_reviews; r = fetch_reviews('$APP_ID'); print(f'Fetched {len(r)} reviews')"
```

**Success Checklist:**
- [ ] Reviews fetched and saved to raw-reviews.json
- [ ] Count and average rating printed

#### Step 4: Build the Theme Clusterer

```bash
# Tell Claude Code:
# "Add a cluster_themes(reviews) function to analyzer.py that:
#   1. Reads reviews from raw-reviews.json
#   2. Sends them to Claude in batches of 50 (to fit context)
#   3. Asks Claude to identify recurring themes: for each theme return
#      {theme_name, count, sentiment (positive/negative/mixed),
#       top_3_quotes, actionability (what the PM should do)}
#   4. Merges and deduplicates themes across batches
#   5. Returns themes sorted by count descending"

python3 -c "from analyzer import cluster_themes; themes = cluster_themes([]); print(f'Found {len(themes)} themes')"
```

**Success Checklist:**
- [ ] Clustering runs without API errors
- [ ] Returns at least 5 distinct themes

#### Step 5: Generate the Report

```bash
# Tell Claude Code:
# "Add a generate_report(themes, app_id) function that writes a markdown file:
#   - Header with app ID, date, total reviews analyzed, avg rating
#   - Executive Summary: top 3 things to fix, top 3 things working well
#   - Themes section: each theme with count, sentiment badge, quotes, action item
#   - Low-hanging fruit section: themes with negative sentiment + high count
#   Save to review-analysis-{date}.md"

python3 -c "
from analyzer import fetch_reviews, cluster_themes, generate_report
reviews = fetch_reviews('$APP_ID')
themes = cluster_themes(reviews)
generate_report(themes, '$APP_ID')
print('Done! Check review-analysis-*.md')
"
```

**Success Checklist:**
- [ ] Report file generated
- [ ] Executive summary is useful and specific
- [ ] At least 8 themes identified with quotes

### ðŸŽ¯ Success Criteria

- [ ] Script fetches 100+ real reviews from the App Store
- [ ] Report identifies at least 6 distinct themes with sentiment
- [ ] Executive summary has concrete, actionable recommendations
- [ ] You can re-run with a different app ID to compare competitors

### ðŸ° Next Steps (Optional)

- Run the same analysis on a competitor's app and compare themes
- Add Google Play support: `pip install google-play-scraper`
- Schedule a monthly run and track how themes shift over time

---

## Mission 3: Build an Automated Changelog Generator

**â±ï¸ Time:** 30â€“40 minutes
**ðŸ“Š Difficulty:** Easy
**ðŸ› ï¸ Tools:** Claude Code, git, Python, Anthropic API

### ðŸ’¡ What You're Building

A tool that reads your git history and rewrites it as a user-facing changelog â€” translating developer commit messages like `fix(auth): handle edge case in token refresh` into things users actually understand like "Fixed an issue where some users were unexpectedly logged out."

**You'll have:**
- Script that parses git log from any repo
- Claude-powered rewriting of technical commits into user-friendly language
- Categorized output: New Features Â· Improvements Â· Bug Fixes
- A clean `CHANGELOG.md` ready to publish

### âœ… Prerequisites

- Python 3.8+ installed
- A git repository with at least 10 commits
- Anthropic API key

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Tool

```bash
mkdir -p ~/pm-tools/changelog-gen
cd ~/pm-tools/changelog-gen
python3 -m venv .venv
source .venv/bin/activate
pip install anthropic gitpython python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch generator.py
```

**Success Checklist:**
- [ ] gitpython installed (lets us read git history from Python)
- [ ] .env has your API key

#### Step 2: Extract Git History

```bash
# Tell Claude Code:
# "Write generator.py with get_commits(repo_path, since_days=30) that:
#   1. Opens the git repo at repo_path using gitpython
#   2. Gets all commits from the last {since_days} days
#   3. For each commit returns: {hash, message, author, date, files_changed}
#   4. Filters out merge commits and 'bump version' style commits
#   5. Prints a summary: how many commits, date range, top 3 authors"

# Test it on a real repo â€” use this project or any local git repo:
python3 -c "
from generator import get_commits
commits = get_commits('/path/to/your/repo', since_days=30)
print(f'Found {len(commits)} commits')
for c in commits[:3]:
    print(f'  {c[\"date\"][:10]} â€” {c[\"message\"][:60]}')
"
```

**Success Checklist:**
- [ ] Script reads commits from a real repo
- [ ] Merge commits filtered out
- [ ] Date range and count printed correctly

#### Step 3: Rewrite Commits for Users

```bash
# Tell Claude Code:
# "Add rewrite_commit(commit) to generator.py that sends the commit message
#  to Claude with this prompt:
#  'You are writing a user-facing changelog. Rewrite this developer commit
#   message in plain English for a non-technical user. Be specific about
#   what changed and why it matters. Do not mention technical terms like
#   refactor, hotfix, or merge. If the commit is too minor to mention
#   (typo fix, test update, CI change), return null.
#   Commit: {message}
#   Files changed: {files_changed}'
#  Return: {user_message, category} where category is one of:
#  'New Feature', 'Improvement', 'Bug Fix', or null"
```

**Success Checklist:**
- [ ] Claude rewrites a test commit in plain language
- [ ] Minor commits correctly return null (filtered out)
- [ ] Categories assigned correctly

#### Step 4: Generate the Changelog File

```bash
# Tell Claude Code:
# "Add generate_changelog(repo_path, since_days=30, version=None) that:
#   1. Calls get_commits() then rewrite_commit() on each (with progress bar)
#   2. Groups results by category
#   3. Writes CHANGELOG.md with:
#      - Version/date header
#      - Sections: âœ¨ New Features, ðŸ”§ Improvements, ðŸ› Bug Fixes
#      - Each item as a bullet point in user language
#   4. Also prints a one-paragraph executive summary to terminal"

python3 -c "
from generator import generate_changelog
generate_changelog('/path/to/your/repo', since_days=30, version='v2.4')
"
cat CHANGELOG.md
```

**Success Checklist:**
- [ ] CHANGELOG.md written to disk
- [ ] At least 2 categories populated
- [ ] Language is genuinely user-friendly (no jargon)
- [ ] Executive summary printed to terminal

### ðŸŽ¯ Success Criteria

- [ ] Script reads real commits from your git repo
- [ ] Output is readable by a non-technical stakeholder
- [ ] Changelog categories are correctly assigned
- [ ] You can run it on any repo by changing the path

### ðŸ° Next Steps (Optional)

- Post the changelog automatically to Slack: `pip install slack-sdk`
- Integrate with your CI/CD pipeline to auto-generate on each release
- Add a `--since-tag v2.3` flag to generate changelogs between releases

---

## Mission 4: Build a User Interview Analysis Pipeline

**â±ï¸ Time:** 45â€“65 minutes
**ðŸ“Š Difficulty:** Intermediate
**ðŸ› ï¸ Tools:** Claude Code, Python, Anthropic API, markdown

### ðŸ’¡ What You're Building

A pipeline that takes a folder of raw user interview transcripts and produces a structured insight report: themes ranked by frequency, pain points with severity scores, representative quotes, jobs-to-be-done, and recommended product bets â€” the output you'd normally spend a full day synthesizing by hand.

**You'll have:**
- A `transcripts/` folder you can drop any .txt or .md file into
- Pipeline that processes all transcripts in one run
- Structured `insights-{date}.md` with themes, pain points, and JTBD
- An executive summary with 3 recommended product bets

### âœ… Prerequisites

- Python 3.8+ installed
- Anthropic API key
- At least 3 user interview transcripts (text files â€” even rough notes work)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Workspace

```bash
mkdir -p ~/pm-tools/interview-analyzer/transcripts
cd ~/pm-tools/interview-analyzer
python3 -m venv .venv
source .venv/bin/activate
pip install anthropic python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch pipeline.py
```

Add your interview transcripts to the `transcripts/` folder as `.txt` or `.md` files. They can be raw â€” verbatim transcripts, rough notes, whatever format you have.

**Success Checklist:**
- [ ] transcripts/ folder has at least 3 files
- [ ] API key in .env

#### Step 2: Build the Transcript Reader

```bash
# Tell Claude Code:
# "Write pipeline.py with load_transcripts(folder='transcripts') that:
#   1. Reads all .txt and .md files in the folder
#   2. For each file: extract participant_id (filename without extension),
#      and splits transcript into chunks of ~2000 words each
#      (so long interviews fit in Claude's context)
#   3. Returns a list of {participant_id, chunk_index, text} dicts
#   4. Prints: how many files loaded, total words across all transcripts"

python3 -c "
from pipeline import load_transcripts
chunks = load_transcripts()
print(f'Loaded {len(chunks)} chunks from transcripts')
"
```

**Success Checklist:**
- [ ] All transcripts loaded
- [ ] Long transcripts correctly chunked
- [ ] Word count printed

#### Step 3: Extract Themes and Pain Points Per Interview

```bash
# Tell Claude Code:
# "Add extract_insights(chunk) to pipeline.py that sends each chunk to Claude
#  with this system prompt:
#  'You are a senior UX researcher analyzing a user interview.
#   Extract structured insights in JSON with these fields:
#   - pain_points: [{description, severity 1-5, verbatim_quote}]
#   - themes: [{name, description, evidence_quote}]
#   - jobs_to_be_done: [string] (what the user is trying to accomplish)
#   - workarounds: [string] (hacks they use today)
#   - delight_moments: [string] (things they love)
#   Return only valid JSON, no commentary.'
#  Use claude-3-5-haiku-20241022 (fast + cheap for per-chunk extraction)"
```

**Success Checklist:**
- [ ] Extraction runs on one chunk without errors
- [ ] Returns valid JSON with all fields
- [ ] Pain points have severity scores and real quotes

#### Step 4: Aggregate Across All Interviews

```bash
# Tell Claude Code:
# "Add aggregate_insights(all_insights) that:
#   1. Takes a list of per-chunk insight dicts
#   2. Deduplicates and merges similar themes (use Claude to do this â€”
#      send all themes to Claude and ask it to merge semantically similar ones
#      and return a ranked list with frequency count)
#   3. Does the same for pain points (merge similar, sum severity)
#   4. Collects all JTBD and deduplicates
#   5. Returns {themes_ranked, pain_points_ranked, jtbd_list, workarounds}"
```

**Success Checklist:**
- [ ] Themes merged (no duplicates like "slow loading" and "app is slow")
- [ ] Pain points ranked by severity Ã— frequency
- [ ] JTBD list is distinct and actionable

#### Step 5: Generate the Insight Report

```bash
# Tell Claude Code:
# "Add generate_report(aggregated) that writes insights-{date}.md with:
#   - Executive Summary: 3 recommended product bets based on findings
#   - Top Pain Points table: rank | description | severity | frequency | quote
#   - Themes: each theme with description, frequency, 2 representative quotes
#   - Jobs to Be Done: bulleted list
#   - Workarounds: what users do today that your product could replace
#   - Raw data appendix: which participant mentioned which theme"

python3 pipeline.py  # run the full pipeline
cat insights-$(date +%Y-%m-%d).md
```

**Success Checklist:**
- [ ] Report generated with all sections
- [ ] Product bets section has specific, defensible recommendations
- [ ] Can be shared directly in a product review meeting

### ðŸŽ¯ Success Criteria

- [ ] Pipeline processes all transcripts in one command
- [ ] At least 5 distinct themes identified across interviews
- [ ] Pain points have severity scores and real verbatim quotes
- [ ] Executive summary recommends specific product bets with reasoning

### ðŸ° Next Steps (Optional)

- Add support for audio transcription: `pip install openai-whisper` â€” point at .mp3 files
- Track themes over time by running monthly and comparing reports
- Feed the insight report into Mission 6 (PM Copilot) as a PRD input

---

## Mission 5: Build a Weekly PM Metrics Dashboard

**â±ï¸ Time:** 60â€“90 minutes
**ðŸ“Š Difficulty:** Advanced
**ðŸ› ï¸ Tools:** Claude Code, Python, pandas, matplotlib, HTML/CSS

### ðŸ’¡ What You're Building

A tool that takes any CSV export from your analytics tool (Mixpanel, Amplitude, Postgres export, a spreadsheet) and generates a self-contained HTML dashboard with charts, trend lines, and auto-written week-over-week commentary. One command, sharable output.

**You'll have:**
- Script that auto-detects date columns and metric columns from any CSV
- Charts for each key metric with 8-week trend lines
- Week-over-week change with green/red indicators and automated commentary
- Self-contained `dashboard-week-{N}.html` you can email or post without any server

### âœ… Prerequisites

- Python 3.8+ installed
- Anthropic API key
- A CSV file with at least 4 weeks of data (any metrics â€” DAU, revenue, signups, etc.)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

```bash
mkdir -p ~/pm-tools/metrics-dashboard
cd ~/pm-tools/metrics-dashboard
python3 -m venv .venv
source .venv/bin/activate
pip install pandas matplotlib anthropic python-dotenv jinja2
echo "ANTHROPIC_API_KEY=your_key_here" > .env
# Drop your CSV file into this folder
ls *.csv  # verify your data file is here
touch dashboard.py
```

**Success Checklist:**
- [ ] All packages installed
- [ ] Your CSV file is in the folder

#### Step 2: Build the CSV Analyzer

```bash
# Tell Claude Code:
# "Write dashboard.py with analyze_csv(filepath) that:
#   1. Reads the CSV with pandas
#   2. Auto-detects the date column (look for 'date', 'week', 'period' column names,
#      or columns with datetime-parseable values)
#   3. Auto-detects numeric metric columns (exclude IDs and string columns)
#   4. Normalizes to weekly data if daily (sum or average per week based on column name:
#      'users', 'revenue', 'conversions' â†’ sum; 'rate', 'score', 'pct' â†’ average)
#   5. Returns a dict: {date_col, metrics: {name: [weekly_values]}, weeks: [date_strings]}
#   6. Prints a preview: what columns were found and what they contain"

python3 -c "from dashboard import analyze_csv; data = analyze_csv('your-file.csv'); print(data['metrics'].keys())"
```

**Success Checklist:**
- [ ] Date column correctly identified
- [ ] At least 3 metric columns detected
- [ ] Data normalized to weekly correctly

#### Step 3: Generate Charts

```bash
# Tell Claude Code:
# "Add generate_charts(data) to dashboard.py that:
#   1. For each metric, creates a matplotlib line chart with:
#      - 8-week trend line in violet (#7c3aed)
#      - Last week highlighted with a dot
#      - WoW change annotation (e.g., '+12.3%' in green or '-4.1%' in red)
#      - Clean minimal style (no gridlines, light gray axes)
#   2. Saves each chart as a base64-encoded PNG string (so HTML is self-contained)
#   3. Returns {metric_name: base64_string} dict"
```

**Success Checklist:**
- [ ] Charts generate without errors
- [ ] WoW change correct and color-coded
- [ ] Base64 encoding works (charts can embed in HTML)

#### Step 4: Auto-Write Commentary with Claude

```bash
# Tell Claude Code:
# "Add write_commentary(data) to dashboard.py that sends the weekly numbers
#  to Claude and asks it to write:
#  - A 2-sentence executive summary of this week's performance
#  - For each metric: one sentence noting what changed and any notable pattern
#    (e.g., 'DAU grew 8% WoW, continuing a 4-week upward trend since the Jan launch')
#  - A 'Watch closely' section flagging any metric down >10% WoW
#  Return as a dict: {summary, metric_commentary: {name: sentence}, watch_list: [name]}"
```

**Success Checklist:**
- [ ] Commentary is specific (mentions real numbers, not generic)
- [ ] Watch list correctly flags declining metrics

#### Step 5: Build and Save the HTML Dashboard

```bash
# Tell Claude Code:
# "Add build_html(data, charts, commentary) that generates a self-contained HTML file:
#   - Dark header with 'Week N â€” YYYY-MM-DD' and executive summary
#   - Metric cards grid: each card has the chart image, current value,
#     WoW change badge (green/red), and the one-sentence commentary
#   - Watch Closely section at bottom with red border
#   - Styling: white background, violet accents, Inter font from Google Fonts
#   - Save as dashboard-week-{week_number}.html"

python3 -c "
from dashboard import analyze_csv, generate_charts, write_commentary, build_html
data = analyze_csv('your-file.csv')
charts = generate_charts(data)
commentary = write_commentary(data)
build_html(data, charts, commentary)
print('Dashboard saved!')
"
open dashboard-week-*.html  # opens in browser
```

**Success Checklist:**
- [ ] HTML file opens in browser without errors
- [ ] All charts render correctly (base64 images)
- [ ] Commentary is accurate and readable
- [ ] File is fully self-contained (can email as attachment)

### ðŸŽ¯ Success Criteria

- [ ] Dashboard generates from your real CSV in one command
- [ ] All metric charts render with correct WoW comparisons
- [ ] Commentary mentions real numbers (not placeholder text)
- [ ] HTML file opens and renders correctly in a browser without any server

### ðŸ° Next Steps (Optional)

- Add a comparison view showing this week vs. 4 weeks ago vs. same week last year
- Hook it into Slack to auto-post the dashboard every Monday: `pip install slack-sdk`
- Extend to pull data directly from Amplitude or Mixpanel APIs instead of CSV export

---

## Mission 6: Build a PM Copilot MCP Server

**â±ï¸ Time:** 2â€“3 hours
**ðŸ“Š Difficulty:** Expert
**ðŸ› ï¸ Tools:** Claude Code, Python, MCP SDK, file system

### ðŸ’¡ What You're Building

A persistent MCP server that lives in Claude Code and gives you 5 PM superpowers as native tools â€” callable from any Claude Code session, forever. Write structured PRDs, analyze feedback folders, snapshot competitor pages, prioritize feature lists, and summarize metrics â€” all without leaving your editor and all saving to a persistent `~/pm-copilot/` workspace.

**You'll have:**
- Fully registered MCP server with 5 PM tools
- `~/pm-copilot/` workspace that accumulates your work over time
- `write_prd` â€” turns a feature idea into a full PRD file
- `analyze_feedback` â€” runs the review analyzer on any folder
- `competitive_snapshot` â€” scrapes + saves competitor pages on demand
- `prioritize_features` â€” scores a list against ICE framework
- `metrics_summary` â€” generates commentary from a CSV in seconds

### âœ… Prerequisites

- Python 3.8+ installed
- Anthropic API key
- Claude Code installed and running
- Completed at least Missions 2 or 3 (familiar with the tools you'll wrap)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Scaffold the MCP Server

```bash
mkdir -p ~/pm-copilot/{prds,feedback,competitive,metrics,features}
cd ~/pm-copilot
python3 -m venv .venv
source .venv/bin/activate
pip install mcp anthropic requests beautifulsoup4 pandas python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch server.py
```

**Success Checklist:**
- [ ] All 5 subdirectories created
- [ ] mcp package installed (run `python3 -c "import mcp; print(mcp.__version__)"`)
- [ ] .env has your API key

#### Step 2: Build the MCP Server Scaffold

```bash
# Tell Claude Code:
# "Write server.py as a complete MCP server using the mcp Python SDK.
#  Use FastMCP for simplicity. Set server name to 'pm-copilot'.
#  Add a placeholder tool called ping() that returns 'PM Copilot is running!'
#  Add proper error handling and logging.
#  At the bottom, run the server with mcp.run()"

# Test the scaffold:
python3 server.py &
sleep 2
echo "Server started"
kill %1
```

**Success Checklist:**
- [ ] Server starts without errors
- [ ] No import failures

#### Step 3: Add the PRD Writer Tool

```bash
# Tell Claude Code:
# "Add a @mcp.tool() write_prd(feature_name, context, user_problem, success_metrics)
#  function to server.py that:
#  1. Sends the inputs to Claude with a PM system prompt that writes a structured PRD:
#     sections: Overview, Problem Statement, Goals & Non-Goals, User Stories,
#     Requirements, Success Metrics, Open Questions, Timeline
#  2. Saves the PRD to ~/pm-copilot/prds/{feature-name-slug}-{date}.md
#  3. Returns the file path and first 200 chars of the PRD as confirmation"

# Test it from Claude Code after server is registered:
# write_prd("offline mode", "mobile app for field workers", "users lose data when connectivity drops", "zero data loss incidents in field")
```

**Success Checklist:**
- [ ] Tool defined with correct @mcp.tool() decorator
- [ ] PRD file saved to correct path
- [ ] PRD has all 8 sections

#### Step 4: Add the Feedback Analyzer Tool

```bash
# Tell Claude Code:
# "Add analyze_feedback(source_dir) tool that:
#  1. Reads all .txt and .md files from source_dir
#  2. Runs the same theme extraction logic from Mission 2
#     (batch Claude calls to extract themes and pain points)
#  3. Saves output to ~/pm-copilot/feedback/analysis-{date}.md
#  4. Returns: {themes_count, top_pain_point, top_theme, output_file}"
```

**Success Checklist:**
- [ ] Tool reads files from any directory path
- [ ] Produces a real analysis (not placeholder)
- [ ] Output file saved correctly

#### Step 5: Add the Competitive Snapshot Tool

```bash
# Tell Claude Code:
# "Add competitive_snapshot(urls, label) tool that:
#  1. Takes a list of URLs and a label (e.g., 'Q1-2026-pricing')
#  2. Fetches each URL with requests
#  3. Extracts main content (strip nav, footer, scripts)
#  4. Saves to ~/pm-copilot/competitive/{label}-{date}.md with one section per URL
#  5. Returns: {pages_captured, output_file, key_differences summary from Claude}"
```

**Success Checklist:**
- [ ] Fetches and saves at least 2 URLs
- [ ] Output is clean markdown (no HTML noise)
- [ ] Key differences summary generated by Claude

#### Step 6: Add Feature Prioritizer and Metrics Summary Tools

```bash
# Tell Claude Code:
# "Add two more tools to server.py:
#
#  1. prioritize_features(features: list[str], context: str)
#     - Scores each feature using ICE: Impact (1-10), Confidence (1-10), Ease (1-10)
#     - Ask Claude to score each with reasoning
#     - Returns ranked list with scores and brief rationale per feature
#     - Saves to ~/pm-copilot/features/prioritization-{date}.md
#
#  2. metrics_summary(csv_path: str, period: str = 'weekly')
#     - Reads the CSV at csv_path
#     - Asks Claude to identify the 3 most important trends
#     - Returns a 3-bullet executive summary with specific numbers
#     - Saves to ~/pm-copilot/metrics/summary-{date}.md"
```

**Success Checklist:**
- [ ] Both tools defined and importable
- [ ] ICE scores are specific (not all 7s)
- [ ] Metrics summary mentions real numbers from the CSV

#### Step 7: Register with Claude Code and Run End-to-End

```bash
# Add server to Claude Code config:
cat >> ~/.claude/claude.json << 'EOF'

# Add to your mcpServers section:
# "pm-copilot": {
#   "command": "python3",
#   "args": ["/Users/your-username/pm-copilot/server.py"],
#   "env": {"ANTHROPIC_API_KEY": "your_key_here"}
# }
EOF

# Tell Claude Code:
# "Update ~/.claude/claude.json to add pm-copilot MCP server at ~/pm-copilot/server.py"

# Restart Claude Code, then test all 5 tools:
# write_prd("dark mode", "settings page", "users work at night", "user satisfaction +10pts")
# prioritize_features(["push notifications", "offline mode", "dark mode", "bulk export"], "B2B mobile app")
```

**Success Checklist:**
- [ ] Server registers in Claude Code without errors
- [ ] All 5 tools appear in Claude Code's tool list
- [ ] write_prd creates a real PRD file
- [ ] prioritize_features returns differentiated scores (not uniform)
- [ ] ~/pm-copilot/ folder accumulating real work artifacts

### ðŸŽ¯ Success Criteria

- [ ] MCP server starts and stays running
- [ ] All 5 tools callable from Claude Code
- [ ] Each tool saves a real file to ~/pm-copilot/
- [ ] write_prd produces a complete, structured PRD you'd actually use
- [ ] prioritize_features gives you a ranked list with reasoning you trust
- [ ] You can restart Claude Code and the server is still registered and working

### ðŸ° Next Steps (Optional)

- Add a `daily_brief` tool that aggregates: yesterday's metrics changes, open PRDs, pending feedback â€” delivered as a morning summary
- Add a `search_workspace(query)` tool that semantic-searches everything in ~/pm-copilot/
- Expose the server on your local network so teammates can use it too: `uvicorn server:app --host 0.0.0.0`
