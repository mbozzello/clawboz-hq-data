# üéØ Launchpad PM Skills - 2026-02-18

**Today's hands-on projects:** 6 PM skills ‚Äî beginner to expert
**Sources:** Launchpad Original
**Difficulty:** Beginner ¬∑ Easy ¬∑ Easy ¬∑ Intermediate ¬∑ Advanced ¬∑ Expert

---

## Mission 1: Scrape and Compare Competitor Pricing Pages

**‚è±Ô∏è Time:** 20‚Äì30 minutes
**üìä Difficulty:** Beginner
**üõ†Ô∏è Tools:** Claude Code, Python, requests, BeautifulSoup

### üí° What You're Building

A script that fetches your competitors' pricing pages and outputs a clean, structured markdown report you can paste into a doc or share in Slack ‚Äî no manual copy-paste required.

**You'll have:**
- Python script that fetches any list of pricing page URLs
- Auto-extraction of pricing tiers, prices, and feature bullets
- Clean `competitive-pricing-YYYY-MM-DD.md` report saved to disk
- Reusable tool you can re-run weekly in 5 seconds

### ‚úÖ Prerequisites

- Python 3.8+ installed (`python3 --version` to check)
- 3 competitor pricing page URLs ready to go

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Create the Project

Set up a clean workspace for your pricing intelligence tool

```bash
mkdir -p ~/pm-tools/pricing-tracker
cd ~/pm-tools/pricing-tracker
python3 -m venv .venv
source .venv/bin/activate
pip install requests beautifulsoup4 markdownify
touch scraper.py config.json
```

**Success Checklist:**
- [ ] Virtual environment activated (you see `.venv` in prompt)
- [ ] `pip install` completed without errors
- [ ] Both files created

#### Step 2: Configure Your Competitors

Create a config file with the URLs you want to track

```bash
cat > config.json << 'EOF'
{
  "competitors": [
    {"name": "Competitor A", "url": "https://example.com/pricing"},
    {"name": "Competitor B", "url": "https://example2.com/pricing"},
    {"name": "Competitor C", "url": "https://example3.com/pricing"}
  ],
  "output_dir": "reports"
}
EOF
mkdir -p reports
```

Replace the example URLs with your actual competitor pricing pages. Ask Claude Code: "Update config.json with these 3 competitor pricing URLs: [paste your URLs]"

**Success Checklist:**
- [ ] config.json has real competitor URLs
- [ ] reports/ directory created

#### Step 3: Build the Scraper

Ask Claude Code to write the scraping logic

```bash
# Tell Claude Code:
# "Write a scraper.py that:
#  1. Reads competitors from config.json
#  2. Fetches each pricing page with requests (5s timeout, browser User-Agent header)
#  3. Uses BeautifulSoup to extract: all price amounts ($X/mo patterns),
#     plan/tier names (h2, h3 near prices), and feature bullet lists
#  4. Saves a markdown report to reports/competitive-pricing-{today}.md
#     with one section per competitor showing their tiers, prices, and top features
#  5. Prints a summary to terminal when done"

# After Claude writes it, run:
python3 scraper.py
```

**Success Checklist:**
- [ ] Script runs without import errors
- [ ] At least one competitor's page fetched successfully
- [ ] Report file created in reports/

#### Step 4: Review and Refine the Report

Open the report and ask Claude to clean it up

```bash
# Open the report to see what was captured
cat reports/competitive-pricing-$(date +%Y-%m-%d).md

# Ask Claude Code:
# "The report has some noise ‚Äî clean up the markdown, remove navigation text,
#  and add a summary table at the top comparing the pricing tiers side by side"
```

**Success Checklist:**
- [ ] Report is readable and clean
- [ ] Summary comparison table at top
- [ ] Ready to share with your team

### üéØ Success Criteria

- [ ] Script fetches all 3 competitor pages without crashing
- [ ] Report includes pricing tiers and feature lists for each competitor
- [ ] You can re-run it anytime with `python3 scraper.py` and get a fresh report
- [ ] Report is clean enough to paste into a Confluence page or Slack

### üê∞ Next Steps (Optional)

Once you've got the basics working, try:

- Add a cron job to run this weekly: `crontab -e` ‚Üí `0 9 * * 1 cd ~/pm-tools/pricing-tracker && source .venv/bin/activate && python3 scraper.py`
- Track changes over time by diffing reports: ask Claude Code to add a "What changed since last week" section
- Add your own product's pricing to the report for side-by-side comparison

*Inspired by: Reforge - competitive intelligence as a core PM discipline*

---

## Mission 2: Build an App Store Review Theme Extractor

**‚è±Ô∏è Time:** 25‚Äì40 minutes
**üìä Difficulty:** Easy
**üõ†Ô∏è Tools:** Claude Code, Python, app-store-scraper

### üí° What You're Building

A tool that pulls your last 200 App Store reviews, clusters them into themes, and produces a ranked insight report with representative quotes ‚Äî the kind of analysis that normally takes a PM 2 hours of manual reading.

**You'll have:**
- Script that fetches reviews for any iOS app by App Store ID
- Automatic theme clustering (pain points, praise, feature requests)
- Ranked output showing which themes appear most
- A `review-analysis-YYYY-MM-DD.md` you can bring to your next product review

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Your app's App Store ID (find it in the App Store URL: `apps.apple.com/app/id{YOUR_ID}`)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

```bash
mkdir -p ~/pm-tools/review-analyzer
cd ~/pm-tools/review-analyzer
python3 -m venv .venv
source .venv/bin/activate
pip install app-store-scraper anthropic python-dotenv
touch analyzer.py .env
```

**Success Checklist:**
- [ ] All packages installed
- [ ] .env file created (will hold your API key)

#### Step 2: Add Your API Key

```bash
echo "ANTHROPIC_API_KEY=your_key_here" >> .env
# Replace 'your_key_here' with your actual Anthropic API key
# (This powers the theme clustering ‚Äî the scraping is free)
```

**Success Checklist:**
- [ ] .env has real ANTHROPIC_API_KEY

#### Step 3: Fetch Reviews

Ask Claude Code to write the review fetcher

```bash
# Tell Claude Code:
# "Write a Python script analyzer.py with:
#   1. fetch_reviews(app_id, country='us', count=200) using app_store_scraper
#      Returns a list of dicts: {rating, title, review, date}
#   2. Save raw reviews to raw-reviews.json so we can rerun analysis
#      without re-fetching
#   3. Print how many reviews were fetched and average rating"

# Run it with your app ID (example: 284882215 = Facebook):
APP_ID=284882215  # replace with your app ID
python3 -c "from analyzer import fetch_reviews; r = fetch_reviews('$APP_ID'); print(f'Fetched {len(r)} reviews')"
```

**Success Checklist:**
- [ ] Reviews fetched and saved to raw-reviews.json
- [ ] Count and average rating printed

#### Step 4: Build the Theme Clusterer

```bash
# Tell Claude Code:
# "Add a cluster_themes(reviews) function to analyzer.py that:
#   1. Reads reviews from raw-reviews.json
#   2. Sends them to Claude in batches of 50 (to fit context)
#   3. Asks Claude to identify recurring themes: for each theme return
#      {theme_name, count, sentiment (positive/negative/mixed),
#       top_3_quotes, actionability (what the PM should do)}
#   4. Merges and deduplicates themes across batches
#   5. Returns themes sorted by count descending"

python3 -c "from analyzer import cluster_themes; themes = cluster_themes([]); print(f'Found {len(themes)} themes')"
```

**Success Checklist:**
- [ ] Clustering runs without API errors
- [ ] Returns at least 5 distinct themes

#### Step 5: Generate the Report

```bash
# Tell Claude Code:
# "Add a generate_report(themes, app_id) function that writes a markdown file:
#   - Header with app ID, date, total reviews analyzed, avg rating
#   - Executive Summary: top 3 things to fix, top 3 things working well
#   - Themes section: each theme with count, sentiment badge, quotes, action item
#   - Low-hanging fruit section: themes with negative sentiment + high count
#   Save to review-analysis-{date}.md"

python3 -c "
from analyzer import fetch_reviews, cluster_themes, generate_report
reviews = fetch_reviews('$APP_ID')
themes = cluster_themes(reviews)
generate_report(themes, '$APP_ID')
print('Done! Check review-analysis-*.md')
"
```

**Success Checklist:**
- [ ] Report file generated
- [ ] Executive summary is useful and specific
- [ ] At least 8 themes identified with quotes

### üéØ Success Criteria

- [ ] Script fetches 100+ real reviews from the App Store
- [ ] Report identifies at least 6 distinct themes with sentiment
- [ ] Executive summary has concrete, actionable recommendations
- [ ] You can re-run with a different app ID to compare competitors

### üê∞ Next Steps (Optional)

- Run the same analysis on a competitor's app and compare themes
- Add Google Play support: `pip install google-play-scraper`
- Schedule a monthly run and track how themes shift over time

*Inspired by: Product Coalition - using qualitative signals to prioritize roadmap decisions*

---

## Mission 3: Build an Automated Changelog Generator

**‚è±Ô∏è Time:** 30‚Äì40 minutes
**üìä Difficulty:** Easy
**üõ†Ô∏è Tools:** Claude Code, git, Python, Anthropic API

### üí° What You're Building

A tool that reads your git history and rewrites it as a user-facing changelog ‚Äî translating developer commit messages like `fix(auth): handle edge case in token refresh` into things users actually understand like "Fixed an issue where some users were unexpectedly logged out."

**You'll have:**
- Script that parses git log from any repo
- Claude-powered rewriting of technical commits into user-friendly language
- Categorized output: New Features ¬∑ Improvements ¬∑ Bug Fixes
- A clean `CHANGELOG.md` ready to publish

### ‚úÖ Prerequisites

- Python 3.8+ installed
- A git repository with at least 10 commits
- Anthropic API key

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Tool

```bash
mkdir -p ~/pm-tools/changelog-gen
cd ~/pm-tools/changelog-gen
python3 -m venv .venv
source .venv/bin/activate
pip install anthropic gitpython python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch generator.py
```

**Success Checklist:**
- [ ] gitpython installed (lets us read git history from Python)
- [ ] .env has your API key

#### Step 2: Extract Git History

```bash
# Tell Claude Code:
# "Write generator.py with get_commits(repo_path, since_days=30) that:
#   1. Opens the git repo at repo_path using gitpython
#   2. Gets all commits from the last {since_days} days
#   3. For each commit returns: {hash, message, author, date, files_changed}
#   4. Filters out merge commits and 'bump version' style commits
#   5. Prints a summary: how many commits, date range, top 3 authors"

# Test it on a real repo ‚Äî use this project or any local git repo:
python3 -c "
from generator import get_commits
commits = get_commits('/path/to/your/repo', since_days=30)
print(f'Found {len(commits)} commits')
for c in commits[:3]:
    print(f'  {c[\"date\"][:10]} ‚Äî {c[\"message\"][:60]}')
"
```

**Success Checklist:**
- [ ] Script reads commits from a real repo
- [ ] Merge commits filtered out
- [ ] Date range and count printed correctly

#### Step 3: Rewrite Commits for Users

```bash
# Tell Claude Code:
# "Add rewrite_commit(commit) to generator.py that sends the commit message
#  to Claude with this prompt:
#  'You are writing a user-facing changelog. Rewrite this developer commit
#   message in plain English for a non-technical user. Be specific about
#   what changed and why it matters. Do not mention technical terms like
#   refactor, hotfix, or merge. If the commit is too minor to mention
#   (typo fix, test update, CI change), return null.
#   Commit: {message}
#   Files changed: {files_changed}'
#  Return: {user_message, category} where category is one of:
#  'New Feature', 'Improvement', 'Bug Fix', or null"
```

**Success Checklist:**
- [ ] Claude rewrites a test commit in plain language
- [ ] Minor commits correctly return null (filtered out)
- [ ] Categories assigned correctly

#### Step 4: Generate the Changelog File

```bash
# Tell Claude Code:
# "Add generate_changelog(repo_path, since_days=30, version=None) that:
#   1. Calls get_commits() then rewrite_commit() on each (with progress bar)
#   2. Groups results by category
#   3. Writes CHANGELOG.md with:
#      - Version/date header
#      - Sections: ‚ú® New Features, üîß Improvements, üêõ Bug Fixes
#      - Each item as a bullet point in user language
#   4. Also prints a one-paragraph executive summary to terminal"

python3 -c "
from generator import generate_changelog
generate_changelog('/path/to/your/repo', since_days=30, version='v2.4')
"
cat CHANGELOG.md
```

**Success Checklist:**
- [ ] CHANGELOG.md written to disk
- [ ] At least 2 categories populated
- [ ] Language is genuinely user-friendly (no jargon)
- [ ] Executive summary printed to terminal

### üéØ Success Criteria

- [ ] Script reads real commits from your git repo
- [ ] Output is readable by a non-technical stakeholder
- [ ] Changelog categories are correctly assigned
- [ ] You can run it on any repo by changing the path

### üê∞ Next Steps (Optional)

- Post the changelog automatically to Slack: `pip install slack-sdk`
- Integrate with your CI/CD pipeline to auto-generate on each release
- Add a `--since-tag v2.3` flag to generate changelogs between releases

*Inspired by: Lenny's Newsletter - keeping customers informed without writing release notes from scratch*

---

## Mission 4: Build a User Interview Analysis Pipeline

**‚è±Ô∏è Time:** 45‚Äì65 minutes
**üìä Difficulty:** Intermediate
**üõ†Ô∏è Tools:** Claude Code, Python, Anthropic API, markdown

### üí° What You're Building

A pipeline that takes a folder of raw user interview transcripts and produces a structured insight report: themes ranked by frequency, pain points with severity scores, representative quotes, jobs-to-be-done, and recommended product bets ‚Äî the output you'd normally spend a full day synthesizing by hand.

**You'll have:**
- A `transcripts/` folder you can drop any .txt or .md file into
- Pipeline that processes all transcripts in one run
- Structured `insights-{date}.md` with themes, pain points, and JTBD
- An executive summary with 3 recommended product bets

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Anthropic API key
- At least 3 user interview transcripts (text files ‚Äî even rough notes work)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Workspace

```bash
mkdir -p ~/pm-tools/interview-analyzer/transcripts
cd ~/pm-tools/interview-analyzer
python3 -m venv .venv
source .venv/bin/activate
pip install anthropic python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch pipeline.py
```

Add your interview transcripts to the `transcripts/` folder as `.txt` or `.md` files. They can be raw ‚Äî verbatim transcripts, rough notes, whatever format you have.

**Success Checklist:**
- [ ] transcripts/ folder has at least 3 files
- [ ] API key in .env

#### Step 2: Build the Transcript Reader

```bash
# Tell Claude Code:
# "Write pipeline.py with load_transcripts(folder='transcripts') that:
#   1. Reads all .txt and .md files in the folder
#   2. For each file: extract participant_id (filename without extension),
#      and splits transcript into chunks of ~2000 words each
#      (so long interviews fit in Claude's context)
#   3. Returns a list of {participant_id, chunk_index, text} dicts
#   4. Prints: how many files loaded, total words across all transcripts"

python3 -c "
from pipeline import load_transcripts
chunks = load_transcripts()
print(f'Loaded {len(chunks)} chunks from transcripts')
"
```

**Success Checklist:**
- [ ] All transcripts loaded
- [ ] Long transcripts correctly chunked
- [ ] Word count printed

#### Step 3: Extract Themes and Pain Points Per Interview

```bash
# Tell Claude Code:
# "Add extract_insights(chunk) to pipeline.py that sends each chunk to Claude
#  with this system prompt:
#  'You are a senior UX researcher analyzing a user interview.
#   Extract structured insights in JSON with these fields:
#   - pain_points: [{description, severity 1-5, verbatim_quote}]
#   - themes: [{name, description, evidence_quote}]
#   - jobs_to_be_done: [string] (what the user is trying to accomplish)
#   - workarounds: [string] (hacks they use today)
#   - delight_moments: [string] (things they love)
#   Return only valid JSON, no commentary.'
#  Use claude-3-5-haiku-20241022 (fast + cheap for per-chunk extraction)"
```

**Success Checklist:**
- [ ] Extraction runs on one chunk without errors
- [ ] Returns valid JSON with all fields
- [ ] Pain points have severity scores and real quotes

#### Step 4: Aggregate Across All Interviews

```bash
# Tell Claude Code:
# "Add aggregate_insights(all_insights) that:
#   1. Takes a list of per-chunk insight dicts
#   2. Deduplicates and merges similar themes (use Claude to do this ‚Äî
#      send all themes to Claude and ask it to merge semantically similar ones
#      and return a ranked list with frequency count)
#   3. Does the same for pain points (merge similar, sum severity)
#   4. Collects all JTBD and deduplicates
#   5. Returns {themes_ranked, pain_points_ranked, jtbd_list, workarounds}"
```

**Success Checklist:**
- [ ] Themes merged (no duplicates like "slow loading" and "app is slow")
- [ ] Pain points ranked by severity √ó frequency
- [ ] JTBD list is distinct and actionable

#### Step 5: Generate the Insight Report

```bash
# Tell Claude Code:
# "Add generate_report(aggregated) that writes insights-{date}.md with:
#   - Executive Summary: 3 recommended product bets based on findings
#   - Top Pain Points table: rank | description | severity | frequency | quote
#   - Themes: each theme with description, frequency, 2 representative quotes
#   - Jobs to Be Done: bulleted list
#   - Workarounds: what users do today that your product could replace
#   - Raw data appendix: which participant mentioned which theme"

python3 pipeline.py  # run the full pipeline
cat insights-$(date +%Y-%m-%d).md
```

**Success Checklist:**
- [ ] Report generated with all sections
- [ ] Product bets section has specific, defensible recommendations
- [ ] Can be shared directly in a product review meeting

### üéØ Success Criteria

- [ ] Pipeline processes all transcripts in one command
- [ ] At least 5 distinct themes identified across interviews
- [ ] Pain points have severity scores and real verbatim quotes
- [ ] Executive summary recommends specific product bets with reasoning

### üê∞ Next Steps (Optional)

- Add support for audio transcription: `pip install openai-whisper` ‚Äî point at .mp3 files
- Track themes over time by running monthly and comparing reports
- Feed the insight report into Mission 6 (PM Copilot) as a PRD input

*Inspired by: Product Talk - Teresa Torres on continuous discovery and evidence-based opportunity identification*

---

## Mission 5: Build a Weekly PM Metrics Dashboard

**‚è±Ô∏è Time:** 60‚Äì90 minutes
**üìä Difficulty:** Advanced
**üõ†Ô∏è Tools:** Claude Code, Python, pandas, matplotlib, HTML/CSS

### üí° What You're Building

A tool that takes any CSV export from your analytics tool (Mixpanel, Amplitude, Postgres export, a spreadsheet) and generates a self-contained HTML dashboard with charts, trend lines, and auto-written week-over-week commentary. One command, sharable output.

**You'll have:**
- Script that auto-detects date columns and metric columns from any CSV
- Charts for each key metric with 8-week trend lines
- Week-over-week change with green/red indicators and automated commentary
- Self-contained `dashboard-week-{N}.html` you can email or post without any server

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Anthropic API key
- A CSV file with at least 4 weeks of data (any metrics ‚Äî DAU, revenue, signups, etc.)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

```bash
mkdir -p ~/pm-tools/metrics-dashboard
cd ~/pm-tools/metrics-dashboard
python3 -m venv .venv
source .venv/bin/activate
pip install pandas matplotlib anthropic python-dotenv jinja2
echo "ANTHROPIC_API_KEY=your_key_here" > .env
# Drop your CSV file into this folder
ls *.csv  # verify your data file is here
touch dashboard.py
```

**Success Checklist:**
- [ ] All packages installed
- [ ] Your CSV file is in the folder

#### Step 2: Build the CSV Analyzer

```bash
# Tell Claude Code:
# "Write dashboard.py with analyze_csv(filepath) that:
#   1. Reads the CSV with pandas
#   2. Auto-detects the date column (look for 'date', 'week', 'period' column names,
#      or columns with datetime-parseable values)
#   3. Auto-detects numeric metric columns (exclude IDs and string columns)
#   4. Normalizes to weekly data if daily (sum or average per week based on column name:
#      'users', 'revenue', 'conversions' ‚Üí sum; 'rate', 'score', 'pct' ‚Üí average)
#   5. Returns a dict: {date_col, metrics: {name: [weekly_values]}, weeks: [date_strings]}
#   6. Prints a preview: what columns were found and what they contain"

python3 -c "from dashboard import analyze_csv; data = analyze_csv('your-file.csv'); print(data['metrics'].keys())"
```

**Success Checklist:**
- [ ] Date column correctly identified
- [ ] At least 3 metric columns detected
- [ ] Data normalized to weekly correctly

#### Step 3: Generate Charts

```bash
# Tell Claude Code:
# "Add generate_charts(data) to dashboard.py that:
#   1. For each metric, creates a matplotlib line chart with:
#      - 8-week trend line in violet (#7c3aed)
#      - Last week highlighted with a dot
#      - WoW change annotation (e.g., '+12.3%' in green or '-4.1%' in red)
#      - Clean minimal style (no gridlines, light gray axes)
#   2. Saves each chart as a base64-encoded PNG string (so HTML is self-contained)
#   3. Returns {metric_name: base64_string} dict"
```

**Success Checklist:**
- [ ] Charts generate without errors
- [ ] WoW change correct and color-coded
- [ ] Base64 encoding works (charts can embed in HTML)

#### Step 4: Auto-Write Commentary with Claude

```bash
# Tell Claude Code:
# "Add write_commentary(data) to dashboard.py that sends the weekly numbers
#  to Claude and asks it to write:
#  - A 2-sentence executive summary of this week's performance
#  - For each metric: one sentence noting what changed and any notable pattern
#    (e.g., 'DAU grew 8% WoW, continuing a 4-week upward trend since the Jan launch')
#  - A 'Watch closely' section flagging any metric down >10% WoW
#  Return as a dict: {summary, metric_commentary: {name: sentence}, watch_list: [name]}"
```

**Success Checklist:**
- [ ] Commentary is specific (mentions real numbers, not generic)
- [ ] Watch list correctly flags declining metrics

#### Step 5: Build and Save the HTML Dashboard

```bash
# Tell Claude Code:
# "Add build_html(data, charts, commentary) that generates a self-contained HTML file:
#   - Dark header with 'Week N ‚Äî YYYY-MM-DD' and executive summary
#   - Metric cards grid: each card has the chart image, current value,
#     WoW change badge (green/red), and the one-sentence commentary
#   - Watch Closely section at bottom with red border
#   - Styling: white background, violet accents, Inter font from Google Fonts
#   - Save as dashboard-week-{week_number}.html"

python3 -c "
from dashboard import analyze_csv, generate_charts, write_commentary, build_html
data = analyze_csv('your-file.csv')
charts = generate_charts(data)
commentary = write_commentary(data)
build_html(data, charts, commentary)
print('Dashboard saved!')
"
open dashboard-week-*.html  # opens in browser
```

**Success Checklist:**
- [ ] HTML file opens in browser without errors
- [ ] All charts render correctly (base64 images)
- [ ] Commentary is accurate and readable
- [ ] File is fully self-contained (can email as attachment)

### üéØ Success Criteria

- [ ] Dashboard generates from your real CSV in one command
- [ ] All metric charts render with correct WoW comparisons
- [ ] Commentary mentions real numbers (not placeholder text)
- [ ] HTML file opens and renders correctly in a browser without any server

### üê∞ Next Steps (Optional)

- Add a comparison view showing this week vs. 4 weeks ago vs. same week last year
- Hook it into Slack to auto-post the dashboard every Monday: `pip install slack-sdk`
- Extend to pull data directly from Amplitude or Mixpanel APIs instead of CSV export

*Inspired by: Reforge - building a metrics practice that surfaces signal from noise*

---

## Mission 6: Build a PM Copilot MCP Server

**‚è±Ô∏è Time:** 2‚Äì3 hours
**üìä Difficulty:** Expert
**üõ†Ô∏è Tools:** Claude Code, Python, MCP SDK, file system

### üí° What You're Building

A persistent MCP server that lives in Claude Code and gives you 5 PM superpowers as native tools ‚Äî callable from any Claude Code session, forever. Write structured PRDs, analyze feedback folders, snapshot competitor pages, prioritize feature lists, and summarize metrics ‚Äî all without leaving your editor and all saving to a persistent `~/pm-copilot/` workspace.

**You'll have:**
- Fully registered MCP server with 5 PM tools
- `~/pm-copilot/` workspace that accumulates your work over time
- `write_prd` ‚Äî turns a feature idea into a full PRD file
- `analyze_feedback` ‚Äî runs the review analyzer on any folder
- `competitive_snapshot` ‚Äî scrapes + saves competitor pages on demand
- `prioritize_features` ‚Äî scores a list against ICE framework
- `metrics_summary` ‚Äî generates commentary from a CSV in seconds

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Anthropic API key
- Claude Code installed and running
- Completed at least Missions 2 or 3 (familiar with the tools you'll wrap)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Scaffold the MCP Server

```bash
mkdir -p ~/pm-copilot/{prds,feedback,competitive,metrics,features}
cd ~/pm-copilot
python3 -m venv .venv
source .venv/bin/activate
pip install mcp anthropic requests beautifulsoup4 pandas python-dotenv
echo "ANTHROPIC_API_KEY=your_key_here" > .env
touch server.py
```

**Success Checklist:**
- [ ] All 5 subdirectories created
- [ ] mcp package installed (run `python3 -c "import mcp; print(mcp.__version__)"`)
- [ ] .env has your API key

#### Step 2: Build the MCP Server Scaffold

```bash
# Tell Claude Code:
# "Write server.py as a complete MCP server using the mcp Python SDK.
#  Use FastMCP for simplicity. Set server name to 'pm-copilot'.
#  Add a placeholder tool called ping() that returns 'PM Copilot is running!'
#  Add proper error handling and logging.
#  At the bottom, run the server with mcp.run()"

# Test the scaffold:
python3 server.py &
sleep 2
echo "Server started"
kill %1
```

**Success Checklist:**
- [ ] Server starts without errors
- [ ] No import failures

#### Step 3: Add the PRD Writer Tool

```bash
# Tell Claude Code:
# "Add a @mcp.tool() write_prd(feature_name, context, user_problem, success_metrics)
#  function to server.py that:
#  1. Sends the inputs to Claude with a PM system prompt that writes a structured PRD:
#     sections: Overview, Problem Statement, Goals & Non-Goals, User Stories,
#     Requirements, Success Metrics, Open Questions, Timeline
#  2. Saves the PRD to ~/pm-copilot/prds/{feature-name-slug}-{date}.md
#  3. Returns the file path and first 200 chars of the PRD as confirmation"

# Test it from Claude Code after server is registered:
# write_prd("offline mode", "mobile app for field workers", "users lose data when connectivity drops", "zero data loss incidents in field")
```

**Success Checklist:**
- [ ] Tool defined with correct @mcp.tool() decorator
- [ ] PRD file saved to correct path
- [ ] PRD has all 8 sections

#### Step 4: Add the Feedback Analyzer Tool

```bash
# Tell Claude Code:
# "Add analyze_feedback(source_dir) tool that:
#  1. Reads all .txt and .md files from source_dir
#  2. Runs the same theme extraction logic from Mission 2
#     (batch Claude calls to extract themes and pain points)
#  3. Saves output to ~/pm-copilot/feedback/analysis-{date}.md
#  4. Returns: {themes_count, top_pain_point, top_theme, output_file}"
```

**Success Checklist:**
- [ ] Tool reads files from any directory path
- [ ] Produces a real analysis (not placeholder)
- [ ] Output file saved correctly

#### Step 5: Add the Competitive Snapshot Tool

```bash
# Tell Claude Code:
# "Add competitive_snapshot(urls, label) tool that:
#  1. Takes a list of URLs and a label (e.g., 'Q1-2026-pricing')
#  2. Fetches each URL with requests
#  3. Extracts main content (strip nav, footer, scripts)
#  4. Saves to ~/pm-copilot/competitive/{label}-{date}.md with one section per URL
#  5. Returns: {pages_captured, output_file, key_differences summary from Claude}"
```

**Success Checklist:**
- [ ] Fetches and saves at least 2 URLs
- [ ] Output is clean markdown (no HTML noise)
- [ ] Key differences summary generated by Claude

#### Step 6: Add Feature Prioritizer and Metrics Summary Tools

```bash
# Tell Claude Code:
# "Add two more tools to server.py:
#
#  1. prioritize_features(features: list[str], context: str)
#     - Scores each feature using ICE: Impact (1-10), Confidence (1-10), Ease (1-10)
#     - Ask Claude to score each with reasoning
#     - Returns ranked list with scores and brief rationale per feature
#     - Saves to ~/pm-copilot/features/prioritization-{date}.md
#
#  2. metrics_summary(csv_path: str, period: str = 'weekly')
#     - Reads the CSV at csv_path
#     - Asks Claude to identify the 3 most important trends
#     - Returns a 3-bullet executive summary with specific numbers
#     - Saves to ~/pm-copilot/metrics/summary-{date}.md"
```

**Success Checklist:**
- [ ] Both tools defined and importable
- [ ] ICE scores are specific (not all 7s)
- [ ] Metrics summary mentions real numbers from the CSV

#### Step 7: Register with Claude Code and Run End-to-End

```bash
# Add server to Claude Code config:
cat >> ~/.claude/claude.json << 'EOF'

# Add to your mcpServers section:
# "pm-copilot": {
#   "command": "python3",
#   "args": ["/Users/your-username/pm-copilot/server.py"],
#   "env": {"ANTHROPIC_API_KEY": "your_key_here"}
# }
EOF

# Tell Claude Code:
# "Update ~/.claude/claude.json to add pm-copilot MCP server at ~/pm-copilot/server.py"

# Restart Claude Code, then test all 5 tools:
# write_prd("dark mode", "settings page", "users work at night", "user satisfaction +10pts")
# prioritize_features(["push notifications", "offline mode", "dark mode", "bulk export"], "B2B mobile app")
```

**Success Checklist:**
- [ ] Server registers in Claude Code without errors
- [ ] All 5 tools appear in Claude Code's tool list
- [ ] write_prd creates a real PRD file
- [ ] prioritize_features returns differentiated scores (not uniform)
- [ ] ~/pm-copilot/ folder accumulating real work artifacts

### üéØ Success Criteria

- [ ] MCP server starts and stays running
- [ ] All 5 tools callable from Claude Code
- [ ] Each tool saves a real file to ~/pm-copilot/
- [ ] write_prd produces a complete, structured PRD you'd actually use
- [ ] prioritize_features gives you a ranked list with reasoning you trust
- [ ] You can restart Claude Code and the server is still registered and working

### üê∞ Next Steps (Optional)

- Add a `daily_brief` tool that aggregates: yesterday's metrics changes, open PRDs, pending feedback ‚Äî delivered as a morning summary
- Add a `search_workspace(query)` tool that semantic-searches everything in ~/pm-copilot/
- Expose the server on your local network so teammates can use it too: `uvicorn server:app --host 0.0.0.0`

*Inspired by: Claude Code MCP SDK documentation ‚Äî building persistent tools that extend Claude across sessions*

---

## Mission 7: Build an X (Twitter) ‚Üí Slack Bot That Runs in the Cloud

**‚è±Ô∏è Time:** 30-45 minutes
**üìä Difficulty:** Easy
**üõ†Ô∏è Tools:** Claude Code, Node.js, Vercel, X API, Slack Webhooks

### üí° What You're Building

A serverless bot that monitors any X (Twitter) account and automatically posts new tweets as rich cards into a Slack channel ‚Äî running every minute in the cloud via Vercel Cron, no server required.

**You'll have:**
- A live bot posting competitor tweets, investor updates, or industry voices to Slack
- Vercel Cron running every minute ‚Äî no Mac needs to stay on
- Rich Slack cards with avatar, tweet text, images, and a link back to the tweet
- A reusable template you can point at any X account in 30 seconds

### ‚úÖ Prerequisites

- Node.js 18+ installed
- A Vercel account (Pro plan required for per-minute cron ‚Äî $20/mo; hourly cron is free on Hobby)
- An X Developer account (free) ‚Äî get a Bearer Token at developer.x.com
- A Slack workspace where you can create an app and Incoming Webhook

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Get Your X API Bearer Token

Go to [developer.x.com](https://developer.x.com), create a project + app, navigate to **Keys and Tokens**, and generate your **Bearer Token**. Copy it ‚Äî you'll need it in Step 5.

The free X developer tier gives 500,000 tweet reads per month. Running every minute uses ~43,200/month, well within the free limit.

```bash
# Verify your token works ‚Äî replace YOUR_BEARER_TOKEN and any public X handle:
curl "https://api.twitter.com/2/users/by/username/levelsio" \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
# You should see: {"data":{"id":"...","name":"...","username":"levelsio"}}
```

**Success Checklist:**
- [ ] Bearer Token generated and copied
- [ ] curl returns a valid JSON user object

#### Step 2: Create a Slack Incoming Webhook

1. Go to [api.slack.com/apps](https://api.slack.com/apps) ‚Üí **Create New App ‚Üí From Scratch**
2. Name it (e.g. "Tweet Watcher") and select your workspace
3. Click **Incoming Webhooks** in the sidebar ‚Üí toggle on
4. **Add New Webhook to Workspace** ‚Üí pick your target channel ‚Üí **Allow**
5. Copy the Webhook URL (`https://hooks.slack.com/services/XXX/YYY/ZZZ`)

```bash
# Test your webhook ‚Äî replace YOUR_WEBHOOK_URL:
curl -X POST YOUR_WEBHOOK_URL \
  -H 'Content-type: application/json' \
  --data '{"text":"Webhook works! üéâ"}'
# You should see "ok" and a message appear in Slack
```

**Success Checklist:**
- [ ] Webhook URL copied
- [ ] Test message appears in your Slack channel

#### Step 3: Get the Numeric User ID for the Account to Monitor

You need the numeric user ID (not just the handle) to query the X API.

```bash
# Replace HANDLE and YOUR_BEARER_TOKEN:
curl "https://api.twitter.com/2/users/by/username/HANDLE" \
  -H "Authorization: Bearer YOUR_BEARER_TOKEN"
# Copy the "id" value from the response ‚Äî it's a long number like "1096436452576047104"
```

**Success Checklist:**
- [ ] Have the numeric ID for the account(s) you want to monitor

#### Step 4: Ask Claude Code to Scaffold the Project

In Claude Code, prompt:

"Create a Vercel project that monitors X accounts and posts new tweets to Slack. It needs: (1) `api/sync.js` ‚Äî a Vercel serverless function that queries the X API v2 timelines endpoint for each account in an ACCOUNTS array, checks for tweets newer than LOOKBACK_MINUTES minutes, and posts each new tweet to Slack as a rich attachment with the author's name, tweet text, and a link. Use the X API bearer token and Slack webhook URL from environment variables. (2) `vercel.json` ‚Äî configuring a cron job to call `/api/sync` every minute. (3) `package.json` with no external dependencies (use Node.js built-in fetch). (4) `.env.example` showing TWITTER_BEARER_TOKEN and SLACK_WEBHOOK_URL."

```bash
# After Claude Code creates the files:
ls -la api/ vercel.json package.json .env.example
cat api/sync.js
```

**Success Checklist:**
- [ ] `api/sync.js` exists and is readable
- [ ] `vercel.json` has a cron schedule entry
- [ ] No external npm dependencies (built-in fetch only, or minimal)

#### Step 5: Configure Accounts and Test Locally

Update the `ACCOUNTS` array in `api/sync.js` with the account(s) you want to monitor:

```bash
# Edit api/sync.js to set your accounts, e.g.:
# const ACCOUNTS = [
#   { id: "YOUR_NUMERIC_ID", handle: "yourhandle" },
# ];

# Create your .env file:
cat > .env << 'EOF'
TWITTER_BEARER_TOKEN=your_bearer_token_here
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/XXX/YYY/ZZZ
EOF

# Test locally (Node 18+ has built-in fetch):
node -e "
const { createServer } = require('http');
require('dotenv').config();
process.env.NODE_ENV = 'test';
" 2>/dev/null || echo "Install dotenv first: npm install dotenv --save-dev"
```

**Success Checklist:**
- [ ] ACCOUNTS array has at least one entry with real ID and handle
- [ ] `.env` file has both environment variables set

#### Step 6: Deploy to Vercel

```bash
# Push to GitHub first:
git init && git add . && git commit -m "Initial tweet bot"
# Create a GitHub repo and push, then:

# Install Vercel CLI and deploy:
npm install -g vercel
vercel --prod

# Add environment variables in Vercel dashboard:
# Settings ‚Üí Environment Variables:
#   TWITTER_BEARER_TOKEN = your token
#   SLACK_WEBHOOK_URL = your webhook URL
# Then redeploy: vercel --prod
```

**Success Checklist:**
- [ ] Vercel deployment successful (green checkmark)
- [ ] Both environment variables added in Vercel dashboard
- [ ] Project redeployed after adding env vars

#### Step 7: Verify the Cron Is Running

```bash
# Trigger the sync manually to verify it works before waiting for cron:
curl https://your-project.vercel.app/api/sync
# Should return {"ok": true} or similar, and post any recent tweets to Slack

# Check Vercel logs:
# vercel.com/your-username/your-project ‚Üí Logs tab
# You should see a new entry every minute
```

**Success Checklist:**
- [ ] Manual curl to `/api/sync` returns success
- [ ] Recent tweets (if any in the last 2 minutes) appear in Slack
- [ ] Vercel Logs show cron entries firing every minute

### üéØ Success Criteria

- [ ] Bot is live on Vercel with cron running automatically
- [ ] New tweets from your target account appear in Slack within 60 seconds
- [ ] No duplicates (the lookback window dedup is working)
- [ ] You can add a new account by editing `ACCOUNTS` and pushing to GitHub

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Monitor multiple accounts (competitors, investors, key voices in your space)
- Filter tweets by keywords ‚Äî only post if the tweet mentions your company name
- Add a `/tweet-search` Slack slash command to query past tweets
- Switch to hourly cron on Vercel Hobby (free) for less time-sensitive monitoring

*Inspired by: X (Twitter) ‚Üí Slack Bot open-source project*

