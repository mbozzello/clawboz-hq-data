# ðŸŽ¯ Launchpad PM Skills - 2026-02-16

**Today's hands-on projects:** 3 practical missions
**Sources:** Lenny's Newsletter, Product Talk, Mind the Product
**Difficulty:** Mix of beginner to intermediate

---

## Mission 1: Build a Stakeholder Update Email Generator

**â±ï¸ Time:** 20-30 minutes
**ðŸ“Š Difficulty:** Beginner
**ðŸ› ï¸ Tools:** Claude Code, Python

### ðŸ’¡ What You're Building

A script that reads your sprint notes or bullet points and generates polished stakeholder update emails, formatted for different audiences (exec, eng team, customers).

**You'll have:**
- A Python script you run every sprint
- Three audience-specific email templates (exec, team, customer)
- Output saved as `updates/YYYY-MM-DD-stakeholder.md`
- Reusable for every future sprint

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set in your environment
- A text file with your sprint notes (bullet points are fine)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the directory structure and install the Anthropic SDK.

```bash
mkdir -p ~/pm-tools/stakeholder-updates
cd ~/pm-tools/stakeholder-updates
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic
mkdir -p updates
touch generate.py
```

**Success Checklist:**
- [ ] Directory created at `~/pm-tools/stakeholder-updates`
- [ ] Virtual environment active
- [ ] `anthropic` package installed

#### Step 2: Write Your Sprint Notes

Create a sample sprint notes file to use as input.

```bash
cat > sprint-notes.txt << 'EOF'
- Shipped search autocomplete, reduced time-to-first-result by 40%
- Fixed critical checkout bug affecting 3% of mobile users
- Started roadmap for Q2 onboarding redesign
- Talked to 6 customers â€” top complaint: confusing pricing page
- Delayed API v2 to next sprint (dependency on infra team)
- Eng team morale good, two new hires onboarding
EOF
echo "Sprint notes ready"
```

**Success Checklist:**
- [ ] `sprint-notes.txt` exists with real content

#### Step 3: Ask Claude Code to Write the Generator Script

Open Claude Code and prompt it to build `generate.py`:

"Write a Python script that reads `sprint-notes.txt`, then uses the Anthropic API to generate three versions of a stakeholder update email: one for executives (short, outcomes-focused, 150 words max), one for the engineering team (technical, candid, includes blockers), and one for customers (positive, benefit-focused, no internal jargon). Save all three to `updates/YYYY-MM-DD-stakeholder.md` with clear section headers."

```bash
# After Claude Code writes the script, verify it:
python generate.py
ls updates/
```

**Success Checklist:**
- [ ] `generate.py` runs without errors
- [ ] Output file created in `updates/`
- [ ] All three versions look distinct and appropriate

#### Step 4: Review and Save as a Template

Read the output and refine the prompt in `generate.py` to match your voice.

```bash
cat updates/*.md
# Edit generate.py to adjust tone, length, or format to your preference
# Then run again:
python generate.py
```

**Success Checklist:**
- [ ] Executive version is â‰¤ 150 words
- [ ] Team version mentions blockers honestly
- [ ] Customer version has zero internal jargon
- [ ] You'd actually send one of these emails

### ðŸŽ¯ Success Criteria

- [ ] Script runs in under 30 seconds
- [ ] Generates three distinct, audience-appropriate updates from bullet points
- [ ] Output saved to a dated file you can reference later
- [ ] You've already found one improvement to make it better

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--audience exec` flag to generate just one version
- Pull sprint notes directly from a Notion page via API
- Schedule it to run automatically every Friday at 4pm
- Add a "wins vs risks" section that Claude extracts automatically

*Inspired by: [Mind the Product - stakeholder communication](https://www.mindtheproduct.com/category/product-management/stakeholder-management/)*

---

## Mission 2: Build a Feature Prioritization Scorer

**â±ï¸ Time:** 30-45 minutes
**ðŸ“Š Difficulty:** Easy
**ðŸ› ï¸ Tools:** Claude Code, Python, CSV

### ðŸ’¡ What You're Building

A script that takes a CSV of feature ideas (with columns like impact, effort, confidence) and outputs a RICE-scored, ranked backlog with Claude-generated justifications for each score â€” saved as a ready-to-share markdown table.

**You'll have:**
- A reusable RICE scoring script for any backlog
- Claude-generated rationale for each feature's score
- Output: `backlog-scored-YYYY-MM-DD.md` with a ranked table
- A process you can run before every planning session

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A list of feature ideas (we'll create a sample)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up Project and Sample Data

Create the project and a realistic sample backlog CSV.

```bash
mkdir -p ~/pm-tools/prioritizer
cd ~/pm-tools/prioritizer
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > backlog.csv << 'EOF'
feature,description,reach,impact,confidence,effort
Onboarding checklist,Step-by-step guide for new users,8000,3,80,2
Dark mode,UI theme toggle,12000,1,90,3
CSV export,Export any table to CSV,3000,3,70,1
API webhooks,Real-time event notifications for integrations,500,3,60,5
Search autocomplete,Instant suggestions while typing,10000,2,85,2
Mobile app,Native iOS/Android app,15000,3,40,13
In-app notifications,Notify users of activity without email,9000,2,75,3
EOF
echo "Backlog CSV ready â€” 7 features"
```

**Success Checklist:**
- [ ] `backlog.csv` created with at least 5 features
- [ ] CSV has reach, impact, confidence, effort columns

#### Step 2: Ask Claude Code to Build the Scorer

In Claude Code, prompt:

"Write a Python script `scorer.py` that: (1) reads `backlog.csv`, (2) calculates the RICE score for each row as `(reach * impact * confidence) / effort`, (3) calls the Anthropic API once per feature to generate a 1-sentence justification for why it ranked where it did, (4) sorts by RICE score descending, (5) saves a markdown table to `backlog-scored-YYYY-MM-DD.md` with columns: Rank, Feature, RICE Score, Justification."

```bash
python scorer.py
```

**Success Checklist:**
- [ ] Script runs end-to-end without errors
- [ ] RICE scores calculated correctly
- [ ] Each feature has a Claude-generated justification
- [ ] Output file is a clean, shareable markdown table

#### Step 3: Sanity Check the Rankings

Read the output and see if the rankings match your intuition.

```bash
cat backlog-scored-*.md
```

If something feels off, add your own `notes` column to `backlog.csv` and update the prompt in `scorer.py` to pass those notes to Claude for better-informed justifications.

**Success Checklist:**
- [ ] Top-ranked feature makes sense for your context
- [ ] Justifications are specific, not generic
- [ ] You'd share this table in a planning meeting

#### Step 4: Run on Your Real Backlog

Replace `backlog.csv` with your actual feature list and re-run.

```bash
# Replace backlog.csv contents with your real features
python scorer.py
# Review and share backlog-scored-YYYY-MM-DD.md
```

**Success Checklist:**
- [ ] Real backlog scored and ranked
- [ ] Output ready to paste into Notion, Confluence, or a doc

### ðŸŽ¯ Success Criteria

- [ ] RICE formula applied correctly to all features
- [ ] Every feature has a distinct, accurate justification
- [ ] Output is a clean markdown table you'd actually share
- [ ] Takes less than 2 minutes to run on any backlog

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add ICE and MoSCoW scoring modes with a `--method` flag
- Auto-pull your backlog from Linear or Jira via API
- Generate a "what we're NOT doing and why" section from low-ranked items
- Add a `strategic_fit` column that Claude scores 1-3 based on your strategy doc

*Inspired by: [Lenny's Newsletter - prioritization frameworks](https://www.lennysnewsletter.com/p/how-to-prioritize-product-roadmap)*

---

## Mission 3: Build a Customer Feedback Theme Clusterer

**â±ï¸ Time:** 40-55 minutes
**ðŸ“Š Difficulty:** Intermediate
**ðŸ› ï¸ Tools:** Claude Code, Python, JSON

### ðŸ’¡ What You're Building

A pipeline that ingests raw customer feedback (from CSV, support tickets, or Intercom exports) and clusters it into themes with frequency counts, representative quotes, and a severity rating â€” saved as a structured insight report you'd share in a product review.

**You'll have:**
- A script that processes any volume of feedback in one run
- Theme clusters with top quotes and frequency counts
- A severity rating (P0/P1/P2) for each theme based on signal strength
- Output: `feedback-themes-YYYY-MM-DD.md` ready to paste into a doc

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- Customer feedback in any text format (we'll create a sample)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up Project and Sample Feedback

Create the project and realistic sample feedback data.

```bash
mkdir -p ~/pm-tools/feedback-clusterer
cd ~/pm-tools/feedback-clusterer
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > feedback.json << 'EOF'
[
  {"id": 1, "text": "I can't figure out how to export my data. Spent 20 minutes looking.", "source": "support"},
  {"id": 2, "text": "The pricing page is so confusing. What's included in the Pro plan?", "source": "intercom"},
  {"id": 3, "text": "Export to CSV is broken on Firefox. Nothing happens when I click.", "source": "support"},
  {"id": 4, "text": "Love the product but onboarding took me forever. No clear starting point.", "source": "survey"},
  {"id": 5, "text": "Where is the data export feature? I need to get my data out.", "source": "support"},
  {"id": 6, "text": "I don't understand the difference between the Basic and Pro tiers.", "source": "intercom"},
  {"id": 7, "text": "First-time setup was overwhelming. Too many options with no guidance.", "source": "survey"},
  {"id": 8, "text": "The dashboard is great but I wish I could export reports as PDF.", "source": "nps"},
  {"id": 9, "text": "Pricing is unclear. Had to contact sales to understand what I was paying for.", "source": "intercom"},
  {"id": 10, "text": "Onboarding checklist would really help. I didn't know where to start.", "source": "survey"},
  {"id": 11, "text": "CSV export just downloads an empty file sometimes.", "source": "support"},
  {"id": 12, "text": "Pro plan limits aren't documented anywhere obvious.", "source": "nps"}
]
EOF
echo "12 feedback items loaded"
```

**Success Checklist:**
- [ ] `feedback.json` created with realistic feedback
- [ ] At least 10 items covering a few overlapping themes

#### Step 2: Ask Claude Code to Build the Clustering Pipeline

In Claude Code, prompt:

"Write a Python script `cluster.py` that: (1) reads `feedback.json`, (2) sends all feedback items in a single API call to Claude asking it to group them into 3-5 themes, return each theme with: theme name, count of items, severity (P0=blocking, P1=significant friction, P2=nice-to-have), top 2 representative quotes, and a one-line recommendation for the PM. (3) Saves the output as a markdown report `feedback-themes-YYYY-MM-DD.md` with each theme as a section."

```bash
python cluster.py
```

**Success Checklist:**
- [ ] Script runs without errors
- [ ] 3-5 distinct themes identified
- [ ] Each theme has count, severity, quotes, and recommendation
- [ ] Output file is clean markdown

#### Step 3: Review the Report

Read the output and verify the themes are accurate.

```bash
cat feedback-themes-*.md
```

Check: Are the themes distinct? Does the P0/P1/P2 severity feel right? Are the representative quotes actually representative?

**Success Checklist:**
- [ ] Themes are distinct and named clearly
- [ ] P0 severity assigned to the most-mentioned pain point
- [ ] Quotes are the best examples from the data
- [ ] Recommendations are actionable, not generic

#### Step 4: Run on Real Feedback

Export feedback from your actual tool (Intercom, Zendesk, NPS survey) as CSV or JSON and run the pipeline.

```bash
# If your export is CSV, add a CSV reader step:
# Ask Claude Code to add a --format csv flag that reads CSV instead of JSON
python cluster.py --input my-real-feedback.csv
cat feedback-themes-*.md
```

**Success Checklist:**
- [ ] Real feedback clustered successfully
- [ ] Report is ready to share in your next product review
- [ ] You found at least one insight you didn't already know

### ðŸŽ¯ Success Criteria

- [ ] All feedback items assigned to a theme
- [ ] P0 themes correctly identified as highest priority
- [ ] Each theme has specific, quotable evidence
- [ ] Report is under 2 pages â€” concise enough to actually read

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--watch` mode that re-clusters whenever the feedback file changes
- Pull feedback directly from Intercom or Zendesk via their APIs
- Add trend analysis by comparing this week's themes to last week's
- Auto-post the report to a Slack channel every Monday morning

*Inspired by: [Product Talk - continuous discovery by Teresa Torres](https://www.producttalk.org/2021/08/product-discovery/)*

---
