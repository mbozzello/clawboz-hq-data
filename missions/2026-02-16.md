# üéØ ClawBoz Project Missions - 2026-02-16

**Today's hands-on projects:** 3 practical missions
**Sources:** Product Hunt, HackerNews, GitHub Trending, X, AI News
**Difficulty:** Mix of beginner to advanced

---

## Mission 1: Build a Personal Knowledge Base with MCP and Vector Search

**‚è±Ô∏è Time:** 40-55 minutes  
**üìä Difficulty:** Advanced  
**üõ†Ô∏è Tools:** Claude Code, Python, ChromaDB

### üí° What You're Building

Create an MCP server that indexes all your notes, docs, and bookmarks with semantic search.

**You'll have:**
- Vector database for personal knowledge
- MCP server with semantic search
- Auto-indexing of notes and documents
- Claude can query your entire knowledge base

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Some documents/notes to index

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up Vector Database

Install and configure ChromaDB

```bash
mkdir -p ~/ClawBoz/knowledge-mcp
cd ~/ClawBoz/knowledge-mcp
python3 -m venv .venv
source .venv/bin/activate
pip install mcp chromadb sentence-transformers
mkdir data indices
```

**Success Checklist:**
- [ ] ChromaDB installed
- [ ] Embedding model downloaded
- [ ] Directories created

#### Step 2: Build Document Indexer

Create indexer for various file types

```bash
# Create indexer.py with:
# - index_markdown(file) - parse and chunk MD files
# - index_pdf(file) - extract text from PDFs
# - index_text(file) - plain text files
# - generate_embeddings(text) - create vectors
# - store_in_chroma(chunks, embeddings) - save to DB
```

**Success Checklist:**
- [ ] Can index markdown files
- [ ] PDF extraction works
- [ ] Embeddings generated
- [ ] Chunks stored in ChromaDB

#### Step 3: Implement Search Tools

Create MCP tools for querying knowledge base

```bash
# In server.py, create tools:
# - search(query, limit) - semantic search
# - get_related(doc_id) - find similar documents
# - get_context(query) - retrieve relevant context
# - add_document(path) - index new document
```

**Success Checklist:**
- [ ] Semantic search returns relevant results
- [ ] Related documents found accurately
- [ ] Can add new documents dynamically
- [ ] Context retrieval works for Claude

#### Step 4: Index Your Documents

Populate knowledge base with your files

```bash
# Point indexer at your documents:
python indexer.py ~/Documents
python indexer.py ~/Notes
python indexer.py ~/Downloads/*.pdf
# Verify indexing:
# Check indices/ directory for ChromaDB files
```

**Success Checklist:**
- [ ] All target directories indexed
- [ ] Embeddings stored successfully
- [ ] Can verify document count
- [ ] Search works on indexed content

#### Step 5: Query Through Claude

Use Claude to search your knowledge

```bash
# Register MCP server
# Try queries like:
# 'Find notes about Python async programming'
# 'What have I learned about MCP servers?'
# 'Search my docs for API authentication patterns'
# 'Summarize my notes on vector databases'
```

**Success Checklist:**
- [ ] Claude can search knowledge base
- [ ] Results are relevant and accurate
- [ ] Can retrieve full document context
- [ ] Performance is acceptable

### üéØ Success Criteria

- [ ] Personal documents are indexed and searchable
- [ ] Semantic search returns relevant results
- [ ] Claude can query your entire knowledge base
- [ ] You understand vector embeddings

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add auto-watching for new documents
- Implement tagging and categorization
- Build knowledge graph visualization
- Add multi-language support

---

## Mission 2: Create a Long-Running Agent That Works Overnight

**‚è±Ô∏è Time:** 35-50 minutes  
**üìä Difficulty:** Intermediate  
**üõ†Ô∏è Tools:** Claude Code, Python, systemd/launchd

### üí° What You're Building

Build an autonomous agent that runs background tasks while you sleep and reports results in the morning.

**You'll have:**
- Autonomous agent that runs scheduled tasks
- Morning report delivered via email or dashboard
- Task queue system for background work
- Crash recovery and error logging

### ‚úÖ Prerequisites

- Python 3.8+ installed
- Access to system scheduler (cron/systemd/launchd)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Create Agent Framework

Build the core agent structure

```bash
mkdir -p ~/ClawBoz/overnight-agent
cd ~/ClawBoz/overnight-agent
python3 -m venv .venv
source .venv/bin/activate
pip install schedule apscheduler
touch agent.py tasks.py config.json
```

**Success Checklist:**
- [ ] Project structure created
- [ ] Scheduling library installed
- [ ] Config file ready

#### Step 2: Implement Task System

Create task queue and executor

```bash
# In tasks.py, create:
# - Task base class with execute() method
# - TaskQueue with priority support
# - Built-in tasks: WebScraper, DataFetcher, Analyzer
# - Result logging to JSON
```

**Success Checklist:**
- [ ] Task base class works
- [ ] Task queue manages tasks
- [ ] At least 2 example tasks implemented
- [ ] Results are logged

#### Step 3: Add Scheduling and Error Handling

Make agent run reliably overnight

```bash
# In agent.py, add:
# - Scheduler that runs tasks at specified times
# - Crash recovery (restart on error)
# - Health checks and status logging
# - Graceful shutdown handling
```

**Success Checklist:**
- [ ] Scheduler runs tasks automatically
- [ ] Errors are caught and logged
- [ ] Agent restarts after crashes
- [ ] Status updates are visible

#### Step 4: Set Up System Service

Configure agent to run on boot

```bash
# For macOS (launchd):
# Create ~/Library/LaunchAgents/com.clawboz.agent.plist
# launchctl load ~/Library/LaunchAgents/com.clawboz.agent.plist
#
# For Linux (systemd):
# Create /etc/systemd/system/clawboz-agent.service
# systemctl enable clawboz-agent
# systemctl start clawboz-agent
```

**Success Checklist:**
- [ ] Service file created
- [ ] Agent starts on boot
- [ ] Can view logs
- [ ] Can stop/start/restart agent

#### Step 5: Create Morning Report

Generate and deliver daily summary

```bash
# Add report.py that:
# - Collects all overnight task results
# - Generates markdown summary
# - Saves to ~/ClawBoz/overnight-agent/reports/
# - (Optional) Emails or posts to dashboard
```

**Success Checklist:**
- [ ] Report generator works
- [ ] Summary includes all task results
- [ ] Report saved to file
- [ ] Can schedule report for 7am

### üéØ Success Criteria

- [ ] Agent runs continuously in background
- [ ] Tasks execute on schedule overnight
- [ ] Morning report shows completed work
- [ ] Agent survives crashes and reboots

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add web dashboard for live monitoring
- Implement task dependencies (run B after A completes)
- Add notification system (Slack, Discord, SMS)
- Create task templates for common workflows

---

## Mission 3: Build an MCP Server to Pull Data from Multiple APIs

**‚è±Ô∏è Time:** 25-35 minutes  
**üìä Difficulty:** Beginner  
**üõ†Ô∏è Tools:** Claude Code, Python, MCP SDK

### üí° What You're Building

Create a unified MCP server that aggregates data from GitHub, HackerNews, and RSS feeds into Claude.

**You'll have:**
- Multi-source data aggregator MCP server
- Claude can query GitHub trends, HN top stories, and RSS feeds
- Unified search across all data sources
- Cached responses for faster queries

### ‚úÖ Prerequisites

- Python 3.8+ installed
- GitHub account (for API token)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Create Project Structure

Set up the MCP server project

```bash
mkdir -p ~/ClawBoz/data-mcp
cd ~/ClawBoz/data-mcp
python3 -m venv .venv
source .venv/bin/activate
pip install mcp requests feedparser
touch server.py config.json
```

**Success Checklist:**
- [ ] Project directory created
- [ ] Virtual environment activated
- [ ] Dependencies installed

#### Step 2: Implement Data Source Connectors

Create functions to fetch from each API

```bash
# Ask Claude Code to create:
# - fetch_github_trending() - uses GitHub API
# - fetch_hackernews_top() - uses HN API
# - fetch_rss_feed(url) - parses any RSS feed
# All should return normalized JSON format
```

**Success Checklist:**
- [ ] GitHub trending fetcher works
- [ ] HackerNews top stories fetcher works
- [ ] RSS feed parser works
- [ ] All return consistent data format

#### Step 3: Build MCP Server with Tools

Create MCP server exposing data tools

```bash
# In server.py, create MCP tools:
# - get_github_trending(language, since)
# - get_hackernews_top(limit)
# - get_rss_feed(feed_url)
# - search_all(query) - searches across all sources
```

**Success Checklist:**
- [ ] MCP server class implemented
- [ ] All 4 tools defined and working
- [ ] Error handling added
- [ ] Response caching implemented

#### Step 4: Register and Test

Connect to Claude and test queries

```bash
# Add to ~/.claude/config.json
# Restart Claude Code
# Test: 'What's trending on GitHub in Python?'
# Test: 'Show me top HackerNews stories'
# Test: 'Search for AI news across all sources'
```

**Success Checklist:**
- [ ] MCP server registered
- [ ] Claude can call all tools
- [ ] Queries return real data
- [ ] Cross-source search works

### üéØ Success Criteria

- [ ] Claude can query multiple data sources through one MCP server
- [ ] Data is aggregated and searchable
- [ ] Caching improves performance
- [ ] You understand MCP tool creation

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add more data sources (Reddit, Product Hunt, etc.)
- Implement trending topic detection
- Create scheduled data refresh
- Build summarization and insights

---

