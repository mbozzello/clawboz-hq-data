# üéØ Launchpad PM Skills - 2026-02-17

**Today's hands-on projects:** 3 practical missions
**Sources:** Product Talk, Reforge, Lenny's Newsletter
**Difficulty:** Mix of beginner to advanced

---

## Mission 1: Build a User Interview Insight Extractor

**‚è±Ô∏è Time:** 25-40 minutes
**üìä Difficulty:** Beginner
**üõ†Ô∏è Tools:** Claude Code, Python

### üí° What You're Building

A script that reads raw user interview transcripts (plain text or markdown) from a folder and outputs a structured insight report: key pain points, jobs-to-be-done, notable quotes, and a one-paragraph opportunity summary.

**You'll have:**
- A reusable script for any set of interview transcripts
- A structured report you'd share in a product review meeting
- Output saved as `insights/YYYY-MM-DD-interviews.md`
- A process that turns 2 hours of synthesis into 5 minutes of running a script

### ‚úÖ Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set in your environment
- At least one user interview transcript as a `.txt` or `.md` file

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project and Sample Transcripts

Create the project structure and two sample transcripts to test with.

```bash
mkdir -p ~/pm-tools/interview-extractor/transcripts
mkdir -p ~/pm-tools/interview-extractor/insights
cd ~/pm-tools/interview-extractor
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > transcripts/interview-01.txt << 'EOF'
Interview with: Sarah, Marketing Manager at a 200-person SaaS company
Date: 2026-02-10

Interviewer: Tell me about your current process for sharing product updates with customers.
Sarah: Honestly it's a mess. We use email, Slack, the in-app changelog ‚Äî none of it is consistent. Customers miss things and then complain on support calls.

Interviewer: What's the biggest pain point?
Sarah: Writing the same update three different ways for three different channels. It takes me half a day every release. And my engineer just dumps the Jira ticket titles on me and says "make it readable." I have no idea what half of these mean technically.

Interviewer: What would your ideal process look like?
Sarah: Write it once, have it adapt for each channel automatically. And I want the technical jargon translated for me ‚Äî I shouldn't have to ask the engineer what "refactored the auth middleware" means for customers.
EOF

cat > transcripts/interview-02.txt << 'EOF'
Interview with: Marcus, Product Manager at a fintech startup
Date: 2026-02-12

Interviewer: Walk me through your weekly stakeholder update process.
Marcus: I spend probably 3 hours every Friday writing updates. Separate emails for the exec team, the board summary, and the engineering team update. Total duplication. I copy-paste and then rewrite everything.

Interviewer: What breaks down?
Marcus: The tone. Execs want high-level wins and risks. Engineers want to know what's changing and why. When I'm tired on a Friday I blur them together and get confused feedback.

Interviewer: Have you tried any tools to help?
Marcus: I tried templates but they don't adapt to the actual content. Every sprint is different. I need something that understands context, not just fills in blanks.
EOF

echo "2 sample transcripts created"
```

**Success Checklist:**
- [ ] `transcripts/` folder has at least 2 `.txt` files
- [ ] Python environment is active

#### Step 2: Ask Claude Code to Write the Extractor Script

In Claude Code, prompt:

"Write a Python script `extract.py` that: (1) reads all `.txt` and `.md` files from the `transcripts/` folder, (2) sends them all in one API call to Claude asking it to extract: top 3 pain points (with evidence quotes), 3 jobs-to-be-done in the format 'When I [situation], I want to [motivation], so I can [outcome]', the 3 most quotable lines, and a one-paragraph opportunity summary. (3) Saves the output as `insights/YYYY-MM-DD-interviews.md`."

```bash
python extract.py
cat insights/*.md
```

**Success Checklist:**
- [ ] Script runs without errors
- [ ] Pain points have direct quotes as evidence
- [ ] JTBD statements are in the right format
- [ ] Opportunity summary is specific, not generic

#### Step 3: Drop In Your Real Transcripts

Copy your own user interview transcripts into the `transcripts/` folder (delete the sample ones first) and run again.

```bash
rm transcripts/interview-0*.txt
# Copy your real transcripts into transcripts/
python extract.py
cat insights/*.md
```

**Success Checklist:**
- [ ] Real transcripts processed successfully
- [ ] At least one insight surprises you
- [ ] Report is ready to share in a product review

#### Step 4: Refine the Output Format

If the format doesn't match how your team works, edit the prompt in `extract.py` to change the sections, add a severity rating, or request a "what we heard vs what it means" interpretation layer.

**Success Checklist:**
- [ ] Output format matches your team's review doc format
- [ ] You'd actually use this before your next discovery review

### üéØ Success Criteria

- [ ] Script processes multiple transcripts in a single run
- [ ] Pain points have specific quotes as evidence
- [ ] JTBD statements pass the "when/want/so" format check
- [ ] Report is ready to paste into Notion or a slide deck

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--compare` flag to diff this week's themes against last week's
- Accept audio transcripts from Otter.ai or Grain as direct input
- Auto-tag quotes by theme and export to Dovetail format
- Generate a one-page "what we learned" deck outline from the insights

*Inspired by: Product Talk - Teresa Torres on continuous discovery and structured customer interviews*

---

## Mission 2: Build a Product Spec Generator from a Job Story

**‚è±Ô∏è Time:** 35-50 minutes
**üìä Difficulty:** Easy
**üõ†Ô∏è Tools:** Claude Code, Python, Markdown

### üí° What You're Building

A script that takes a single job story (or problem statement) as input and generates a complete, opinionated product spec ‚Äî including context, non-goals, success metrics, edge cases, and open questions ‚Äî saved as a `specs/YYYY-MM-DD-{feature}.md` file ready for engineering handoff.

**You'll have:**
- A spec generator you run whenever a feature gets greenlit
- A structured spec template that forces you to think through edge cases
- Output: a complete markdown spec in under 60 seconds
- A reusable workflow from "we should build X" to "here's the spec"

### ‚úÖ Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A feature or problem statement you want to spec out

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the project and specs directory.

```bash
mkdir -p ~/pm-tools/spec-generator/specs
cd ~/pm-tools/spec-generator
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic
touch generate_spec.py
echo "Ready"
```

**Success Checklist:**
- [ ] Directory created with `specs/` subfolder
- [ ] Virtual environment active

#### Step 2: Define Your Feature Input File

Create a simple input format ‚Äî a plain text file with the job story and context.

```bash
cat > feature-input.txt << 'EOF'
Feature: CSV Export for any data table
Job story: When I need to share data with a stakeholder who doesn't have access to the app, I want to export any table to CSV, so I can send it without setting up another account.
User type: Admin, Data Analyst
Priority: P1 - multiple customers requesting this quarter
Known constraints: Tables can have up to 50,000 rows; export must be async for large sets
What we're NOT doing: Real-time sync, PDF export, custom column selection (v1)
EOF
echo "Feature input ready"
```

**Success Checklist:**
- [ ] Input file has job story, user type, priority, and constraints

#### Step 3: Ask Claude Code to Build the Spec Generator

In Claude Code, prompt:

"Write a Python script `generate_spec.py` that reads `feature-input.txt` and calls the Anthropic API to generate a complete product spec in markdown. The spec must include these sections: (1) Overview (1 paragraph context), (2) Problem Statement (restated from the job story), (3) Goals and Success Metrics (3 measurable metrics), (4) Non-Goals (explicitly what v1 won't do), (5) User Stories (3-5 in 'As a [user], I want to [action], so that [outcome]' format), (6) Edge Cases and Error States (at least 4), (7) Open Questions (3-5 unresolved decisions), (8) Engineering Notes (suggested approach, not prescriptive). Save to `specs/YYYY-MM-DD-{feature-name}.md`."

```bash
python generate_spec.py
cat specs/*.md
```

**Success Checklist:**
- [ ] Spec generated and saved to `specs/` folder
- [ ] All 8 sections present
- [ ] Success metrics are measurable (not "improve UX")
- [ ] Edge cases are specific (not generic "handle errors")

#### Step 4: Review and Edit the Spec

Read through the generated spec and make any edits before sharing with engineering.

```bash
cat specs/*.md
# Edit directly in your editor ‚Äî the spec is a starting point, not final
```

Key things to check:
- Do the success metrics actually match how you'll measure the feature?
- Are the open questions real blockers or already answered?
- Does the engineering notes section avoid being too prescriptive?

**Success Checklist:**
- [ ] You've read every section and made at least 2 edits
- [ ] The spec is ready to share with engineering for a refinement session

### üéØ Success Criteria

- [ ] Spec generated in under 60 seconds from a job story
- [ ] All 8 required sections present and specific
- [ ] Success metrics are measurable and relevant
- [ ] You'd share this spec with engineering without major rewrites

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--review` flag that critiques an existing spec for missing sections
- Pull the feature input from a Linear issue via API
- Generate a companion "engineering questions" doc at the same time
- Add a DACI section (Decider, Approver, Consulted, Informed)

*Inspired by: Lenny's Newsletter - writing product specs that engineering teams actually understand*

---

## Mission 3: Build a Competitive Intelligence Monitor

**‚è±Ô∏è Time:** 55-80 minutes
**üìä Difficulty:** Advanced
**üõ†Ô∏è Tools:** Claude Code, Python, requests, BeautifulSoup

### üí° What You're Building

A pipeline that scrapes competitor pricing pages, changelog/blog pages, and app store listings on a schedule, compares against last week's snapshot, and outputs a structured competitive diff report ‚Äî highlighting new features, pricing changes, and positioning shifts.

**You'll have:**
- A competitor monitoring script you run weekly (or automate via cron)
- Versioned snapshots of competitor content in `~/pm-tools/competitive/snapshots/`
- A weekly diff report: what changed, what's new, what disappeared
- A process that turns ad-hoc competitive research into a consistent habit

### ‚úÖ Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A list of 2-3 competitor URLs to monitor (pricing page, blog/changelog)

### üöÄ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the directory structure and install dependencies.

```bash
mkdir -p ~/pm-tools/competitive/{snapshots,reports}
cd ~/pm-tools/competitive
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic requests beautifulsoup4

cat > competitors.json << 'EOF'
{
  "competitors": [
    {
      "name": "Competitor A",
      "pages": {
        "pricing": "https://stripe.com/pricing",
        "changelog": "https://stripe.com/blog/changelog"
      }
    },
    {
      "name": "Competitor B",
      "pages": {
        "pricing": "https://linear.app/pricing"
      }
    }
  ]
}
EOF
echo "Competitors config ready"
```

**Success Checklist:**
- [ ] Directory structure created
- [ ] `competitors.json` has at least 2 competitors with real URLs
- [ ] `requests` and `beautifulsoup4` installed

#### Step 2: Ask Claude Code to Write the Scraper and Snapshot System

In Claude Code, prompt:

"Write a Python script `monitor.py` that: (1) reads `competitors.json`, (2) for each competitor page, fetches the HTML, extracts readable text using BeautifulSoup (strip nav/footer/scripts), (3) saves the cleaned text to `snapshots/{competitor-name}-{page-type}-{YYYY-MM-DD}.txt`, (4) if a snapshot from the previous week exists, loads it for comparison, (5) sends both the old and new snapshots to Claude API asking it to identify: new features mentioned, pricing changes, removed features, and messaging/positioning shifts, (6) saves the diff report to `reports/competitive-diff-{YYYY-MM-DD}.md`."

```bash
python monitor.py
cat reports/*.md
```

**Success Checklist:**
- [ ] Script fetches and saves snapshots for all competitors
- [ ] Clean text extracted (no navigation clutter)
- [ ] Diff report generated (on first run: "no previous snapshot" message is fine)

#### Step 3: Run Again the Next Day to See a Real Diff

To simulate a diff without waiting a week, manually edit one of the snapshot files to add a fake change, then re-run.

```bash
# Add a fake change to test the diff:
echo "NEW: We now offer a free tier with unlimited users." >> snapshots/Competitor-A-pricing-*.txt
python monitor.py
cat reports/competitive-diff-*.md
```

**Success Checklist:**
- [ ] Diff report picks up the added change
- [ ] Claude's analysis is specific ("Competitor A added a free tier") not generic
- [ ] Report sections match the expected categories (new features, pricing, messaging)

#### Step 4: Schedule as a Weekly Cron Job

Set up a weekly cron to run every Monday morning so you never miss a competitive change.

```bash
# Add to crontab (runs every Monday at 8am):
(crontab -l 2>/dev/null; echo "0 8 * * 1 cd ~/pm-tools/competitive && .venv/bin/python monitor.py >> logs/monitor.log 2>&1") | crontab -

mkdir -p ~/pm-tools/competitive/logs
echo "Cron job installed. Verify with: crontab -l"
crontab -l | grep monitor
```

**Success Checklist:**
- [ ] Cron entry visible in `crontab -l` output
- [ ] Logs directory created
- [ ] You know where to find next Monday's report

### üéØ Success Criteria

- [ ] Snapshots saved for all competitors after first run
- [ ] Diff report correctly identifies changes between snapshots
- [ ] Report sections are specific enough to share in a competitive review meeting
- [ ] Cron job installed and verified

### üê∞ Next Steps (Optional)

Once you've completed the basics, try:

- Add App Store monitoring (scrape reviews and ratings weekly)
- Track competitor job postings to infer where they're investing
- Auto-post the weekly diff to a Slack channel
- Add email alerts for major changes (pricing page structure changes significantly)

*Inspired by: Reforge - competitive intelligence as a core PM practice*

---
