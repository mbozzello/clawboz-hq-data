# ðŸŽ¯ Launchpad PM Skills - 2026-02-17

**Today's hands-on projects:** 6 practical missions
**Sources:** Product Talk, Reforge, Lenny's Newsletter, @lennysan, @shreyas, @ttorres
**Difficulty:** Mix of beginner to advanced

---

## Mission 1: Build a User Interview Insight Extractor

**â±ï¸ Time:** 25-40 minutes
**ðŸ“Š Difficulty:** Beginner
**ðŸ› ï¸ Tools:** Claude Code, Python

### ðŸ’¡ What You're Building

A script that reads raw user interview transcripts (plain text or markdown) from a folder and outputs a structured insight report: key pain points, jobs-to-be-done, notable quotes, and a one-paragraph opportunity summary.

**You'll have:**
- A reusable script for any set of interview transcripts
- A structured report you'd share in a product review meeting
- Output saved as `insights/YYYY-MM-DD-interviews.md`
- A process that turns 2 hours of synthesis into 5 minutes of running a script

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set in your environment
- At least one user interview transcript as a `.txt` or `.md` file

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project and Sample Transcripts

Create the project structure and two sample transcripts to test with.

```bash
mkdir -p ~/pm-tools/interview-extractor/transcripts
mkdir -p ~/pm-tools/interview-extractor/insights
cd ~/pm-tools/interview-extractor
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > transcripts/interview-01.txt << 'EOF'
Interview with: Sarah, Marketing Manager at a 200-person SaaS company
Date: 2026-02-10

Interviewer: Tell me about your current process for sharing product updates with customers.
Sarah: Honestly it's a mess. We use email, Slack, the in-app changelog â€” none of it is consistent. Customers miss things and then complain on support calls.

Interviewer: What's the biggest pain point?
Sarah: Writing the same update three different ways for three different channels. It takes me half a day every release. And my engineer just dumps the Jira ticket titles on me and says "make it readable." I have no idea what half of these mean technically.

Interviewer: What would your ideal process look like?
Sarah: Write it once, have it adapt for each channel automatically. And I want the technical jargon translated for me â€” I shouldn't have to ask the engineer what "refactored the auth middleware" means for customers.
EOF

cat > transcripts/interview-02.txt << 'EOF'
Interview with: Marcus, Product Manager at a fintech startup
Date: 2026-02-12

Interviewer: Walk me through your weekly stakeholder update process.
Marcus: I spend probably 3 hours every Friday writing updates. Separate emails for the exec team, the board summary, and the engineering team update. Total duplication. I copy-paste and then rewrite everything.

Interviewer: What breaks down?
Marcus: The tone. Execs want high-level wins and risks. Engineers want to know what's changing and why. When I'm tired on a Friday I blur them together and get confused feedback.

Interviewer: Have you tried any tools to help?
Marcus: I tried templates but they don't adapt to the actual content. Every sprint is different. I need something that understands context, not just fills in blanks.
EOF

echo "2 sample transcripts created"
```

**Success Checklist:**
- [ ] `transcripts/` folder has at least 2 `.txt` files
- [ ] Python environment is active

#### Step 2: Ask Claude Code to Write the Extractor Script

In Claude Code, prompt:

"Write a Python script `extract.py` that: (1) reads all `.txt` and `.md` files from the `transcripts/` folder, (2) sends them all in one API call to Claude asking it to extract: top 3 pain points (with evidence quotes), 3 jobs-to-be-done in the format 'When I [situation], I want to [motivation], so I can [outcome]', the 3 most quotable lines, and a one-paragraph opportunity summary. (3) Saves the output as `insights/YYYY-MM-DD-interviews.md`."

```bash
python extract.py
cat insights/*.md
```

**Success Checklist:**
- [ ] Script runs without errors
- [ ] Pain points have direct quotes as evidence
- [ ] JTBD statements are in the right format
- [ ] Opportunity summary is specific, not generic

#### Step 3: Drop In Your Real Transcripts

Copy your own user interview transcripts into the `transcripts/` folder (delete the sample ones first) and run again.

```bash
rm transcripts/interview-0*.txt
# Copy your real transcripts into transcripts/
python extract.py
cat insights/*.md
```

**Success Checklist:**
- [ ] Real transcripts processed successfully
- [ ] At least one insight surprises you
- [ ] Report is ready to share in a product review

#### Step 4: Refine the Output Format

If the format doesn't match how your team works, edit the prompt in `extract.py` to change the sections, add a severity rating, or request a "what we heard vs what it means" interpretation layer.

**Success Checklist:**
- [ ] Output format matches your team's review doc format
- [ ] You'd actually use this before your next discovery review

### ðŸŽ¯ Success Criteria

- [ ] Script processes multiple transcripts in a single run
- [ ] Pain points have specific quotes as evidence
- [ ] JTBD statements pass the "when/want/so" format check
- [ ] Report is ready to paste into Notion or a slide deck

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--compare` flag to diff this week's themes against last week's
- Accept audio transcripts from Otter.ai or Grain as direct input
- Auto-tag quotes by theme and export to Dovetail format
- Generate a one-page "what we learned" deck outline from the insights

*Inspired by: [Product Talk - structured customer interviews by Teresa Torres](https://www.producttalk.org/2022/12/customer-interviews/)*

---

## Mission 2: Build a Product Spec Generator from a Job Story

**â±ï¸ Time:** 35-50 minutes
**ðŸ“Š Difficulty:** Easy
**ðŸ› ï¸ Tools:** Claude Code, Python, Markdown

### ðŸ’¡ What You're Building

A script that takes a single job story (or problem statement) as input and generates a complete, opinionated product spec â€” including context, non-goals, success metrics, edge cases, and open questions â€” saved as a `specs/YYYY-MM-DD-{feature}.md` file ready for engineering handoff.

**You'll have:**
- A spec generator you run whenever a feature gets greenlit
- A structured spec template that forces you to think through edge cases
- Output: a complete markdown spec in under 60 seconds
- A reusable workflow from "we should build X" to "here's the spec"

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A feature or problem statement you want to spec out

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the project and specs directory.

```bash
mkdir -p ~/pm-tools/spec-generator/specs
cd ~/pm-tools/spec-generator
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic
touch generate_spec.py
echo "Ready"
```

**Success Checklist:**
- [ ] Directory created with `specs/` subfolder
- [ ] Virtual environment active

#### Step 2: Define Your Feature Input File

Create a simple input format â€” a plain text file with the job story and context.

```bash
cat > feature-input.txt << 'EOF'
Feature: CSV Export for any data table
Job story: When I need to share data with a stakeholder who doesn't have access to the app, I want to export any table to CSV, so I can send it without setting up another account.
User type: Admin, Data Analyst
Priority: P1 - multiple customers requesting this quarter
Known constraints: Tables can have up to 50,000 rows; export must be async for large sets
What we're NOT doing: Real-time sync, PDF export, custom column selection (v1)
EOF
echo "Feature input ready"
```

**Success Checklist:**
- [ ] Input file has job story, user type, priority, and constraints

#### Step 3: Ask Claude Code to Build the Spec Generator

In Claude Code, prompt:

"Write a Python script `generate_spec.py` that reads `feature-input.txt` and calls the Anthropic API to generate a complete product spec in markdown. The spec must include these sections: (1) Overview (1 paragraph context), (2) Problem Statement (restated from the job story), (3) Goals and Success Metrics (3 measurable metrics), (4) Non-Goals (explicitly what v1 won't do), (5) User Stories (3-5 in 'As a [user], I want to [action], so that [outcome]' format), (6) Edge Cases and Error States (at least 4), (7) Open Questions (3-5 unresolved decisions), (8) Engineering Notes (suggested approach, not prescriptive). Save to `specs/YYYY-MM-DD-{feature-name}.md`."

```bash
python generate_spec.py
cat specs/*.md
```

**Success Checklist:**
- [ ] Spec generated and saved to `specs/` folder
- [ ] All 8 sections present
- [ ] Success metrics are measurable (not "improve UX")
- [ ] Edge cases are specific (not generic "handle errors")

#### Step 4: Review and Edit the Spec

Read through the generated spec and make any edits before sharing with engineering.

```bash
cat specs/*.md
# Edit directly in your editor â€” the spec is a starting point, not final
```

Key things to check:
- Do the success metrics actually match how you'll measure the feature?
- Are the open questions real blockers or already answered?
- Does the engineering notes section avoid being too prescriptive?

**Success Checklist:**
- [ ] You've read every section and made at least 2 edits
- [ ] The spec is ready to share with engineering for a refinement session

### ðŸŽ¯ Success Criteria

- [ ] Spec generated in under 60 seconds from a job story
- [ ] All 8 required sections present and specific
- [ ] Success metrics are measurable and relevant
- [ ] You'd share this spec with engineering without major rewrites

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--review` flag that critiques an existing spec for missing sections
- Pull the feature input from a Linear issue via API
- Generate a companion "engineering questions" doc at the same time
- Add a DACI section (Decider, Approver, Consulted, Informed)

*Inspired by: [Lenny's Newsletter - PRD and 1-pager examples and templates](https://www.lennysnewsletter.com/p/prds-1-pagers-examples)*

---

## Mission 3: Build a Competitive Intelligence Monitor

**â±ï¸ Time:** 55-80 minutes
**ðŸ“Š Difficulty:** Advanced
**ðŸ› ï¸ Tools:** Claude Code, Python, requests, BeautifulSoup

### ðŸ’¡ What You're Building

A pipeline that scrapes competitor pricing pages, changelog/blog pages, and app store listings on a schedule, compares against last week's snapshot, and outputs a structured competitive diff report â€” highlighting new features, pricing changes, and positioning shifts.

**You'll have:**
- A competitor monitoring script you run weekly (or automate via cron)
- Versioned snapshots of competitor content in `~/pm-tools/competitive/snapshots/`
- A weekly diff report: what changed, what's new, what disappeared
- A process that turns ad-hoc competitive research into a consistent habit

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A list of 2-3 competitor URLs to monitor (pricing page, blog/changelog)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the directory structure and install dependencies.

```bash
mkdir -p ~/pm-tools/competitive/{snapshots,reports}
cd ~/pm-tools/competitive
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic requests beautifulsoup4

cat > competitors.json << 'EOF'
{
  "competitors": [
    {
      "name": "Competitor A",
      "pages": {
        "pricing": "https://stripe.com/pricing",
        "changelog": "https://stripe.com/blog/changelog"
      }
    },
    {
      "name": "Competitor B",
      "pages": {
        "pricing": "https://linear.app/pricing"
      }
    }
  ]
}
EOF
echo "Competitors config ready"
```

**Success Checklist:**
- [ ] Directory structure created
- [ ] `competitors.json` has at least 2 competitors with real URLs
- [ ] `requests` and `beautifulsoup4` installed

#### Step 2: Ask Claude Code to Write the Scraper and Snapshot System

In Claude Code, prompt:

"Write a Python script `monitor.py` that: (1) reads `competitors.json`, (2) for each competitor page, fetches the HTML, extracts readable text using BeautifulSoup (strip nav/footer/scripts), (3) saves the cleaned text to `snapshots/{competitor-name}-{page-type}-{YYYY-MM-DD}.txt`, (4) if a snapshot from the previous week exists, loads it for comparison, (5) sends both the old and new snapshots to Claude API asking it to identify: new features mentioned, pricing changes, removed features, and messaging/positioning shifts, (6) saves the diff report to `reports/competitive-diff-{YYYY-MM-DD}.md`."

```bash
python monitor.py
cat reports/*.md
```

**Success Checklist:**
- [ ] Script fetches and saves snapshots for all competitors
- [ ] Clean text extracted (no navigation clutter)
- [ ] Diff report generated (on first run: "no previous snapshot" message is fine)

#### Step 3: Run Again the Next Day to See a Real Diff

To simulate a diff without waiting a week, manually edit one of the snapshot files to add a fake change, then re-run.

```bash
# Add a fake change to test the diff:
echo "NEW: We now offer a free tier with unlimited users." >> snapshots/Competitor-A-pricing-*.txt
python monitor.py
cat reports/competitive-diff-*.md
```

**Success Checklist:**
- [ ] Diff report picks up the added change
- [ ] Claude's analysis is specific ("Competitor A added a free tier") not generic
- [ ] Report sections match the expected categories (new features, pricing, messaging)

#### Step 4: Schedule as a Weekly Cron Job

Set up a weekly cron to run every Monday morning so you never miss a competitive change.

```bash
# Add to crontab (runs every Monday at 8am):
(crontab -l 2>/dev/null; echo "0 8 * * 1 cd ~/pm-tools/competitive && .venv/bin/python monitor.py >> logs/monitor.log 2>&1") | crontab -

mkdir -p ~/pm-tools/competitive/logs
echo "Cron job installed. Verify with: crontab -l"
crontab -l | grep monitor
```

**Success Checklist:**
- [ ] Cron entry visible in `crontab -l` output
- [ ] Logs directory created
- [ ] You know where to find next Monday's report

### ðŸŽ¯ Success Criteria

- [ ] Snapshots saved for all competitors after first run
- [ ] Diff report correctly identifies changes between snapshots
- [ ] Report sections are specific enough to share in a competitive review meeting
- [ ] Cron job installed and verified

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add App Store monitoring (scrape reviews and ratings weekly)
- Track competitor job postings to infer where they're investing
- Auto-post the weekly diff to a Slack channel
- Add email alerts for major changes (pricing page structure changes significantly)

*Inspired by: [Reforge - launching competitor analysis](https://www.reforge.com/blog/launching-competitor-analysis)*

---

## Mission 4: Build an AI Quote Verification Tool for Data Analysis

**â±ï¸ Time:** 45-60 minutes
**ðŸ“Š Difficulty:** Intermediate
**ðŸ› ï¸ Tools:** Claude Code, Python, Claude API

### ðŸ’¡ What You're Building

A tool that takes any AI-generated analysis (a research summary, a market report, a user interview synthesis) and systematically verifies every quote, statistic, and citation against the original source material â€” flagging hallucinations before they end up in your deck.

**You'll have:**
- A quote extraction system that pulls every cited claim from an AI response
- A verification prompt chain that cross-checks each quote against source docs
- A confidence scoring system (verified / unverifiable / fabricated) for each claim
- A formatted report showing verified vs. flagged claims with evidence trails

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A folder of source documents (interview transcripts, reports, articles) and an AI-generated analysis that references them

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project and Sample Data

Create the project and prepare a test case: a source document plus an AI summary that contains a mix of real and hallucinated quotes.

```bash
mkdir -p ~/pm-tools/quote-verifier/{sources,analyses}
cd ~/pm-tools/quote-verifier
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > sources/user-research-notes.txt << 'EOF'
Interview with: Sarah, Marketing Manager, 200-person SaaS
Date: 2026-02-10

Sarah: "Writing the same update three different ways for three different channels takes me half a day every release."
Sarah: "My engineer dumps the Jira ticket titles on me and says 'make it readable.' I have no idea what half of these mean technically."
Sarah: "I want the technical jargon translated for me automatically."

Interview with: Marcus, PM at fintech startup
Date: 2026-02-12

Marcus: "I spend probably 3 hours every Friday writing updates."
Marcus: "Execs want high-level wins and risks. Engineers want to know what's changing and why."
EOF

cat > analyses/ai-summary.txt << 'EOF'
Key findings from user research:

1. Sarah reported that "updating three channels takes her entire day" â€” indicating severe workflow inefficiency.
2. Marcus mentioned spending "5-6 hours per week on stakeholder communications" â€” a significant time sink.
3. Both users expressed frustration with "the lack of automated translation between technical and business language."
4. Sarah specifically noted that "her team has tried four different tools and none worked" for this problem.
EOF

echo "Test data ready â€” note: quotes 2 and 4 in the analysis are fabricated"
```

**Success Checklist:**
- [ ] Source docs and AI analysis files created
- [ ] You can see which quotes in the analysis are real vs. fabricated

#### Step 2: Ask Claude Code to Build the Verifier

In Claude Code, prompt:

"Write a Python script `verify.py` that: (1) reads all files from `sources/` as ground truth, (2) reads an AI analysis from `analyses/`, (3) extracts every quoted statement and specific claim from the analysis, (4) for each quote, sends the quote plus all source docs to Claude API asking: 'Is this quote accurately attributed? Does it appear verbatim or paraphrased in the sources? Rate confidence: VERIFIED (exact match), PARAPHRASED (close but altered), UNVERIFIABLE (not found in sources), FABRICATED (contradicted by sources).' (5) Outputs a verification report to `reports/verification-{timestamp}.md` with each claim, its rating, and the evidence."

```bash
python verify.py
cat reports/*.md
```

**Success Checklist:**
- [ ] Script extracts all 4 claims from the analysis
- [ ] Quote 1 rated PARAPHRASED (close but "half a day" â‰  "entire day")
- [ ] Quote 2 rated FABRICATED (Marcus said "3 hours" not "5-6 hours")
- [ ] Quote 4 rated FABRICATED (Sarah never mentioned trying four tools)

#### Step 3: Test with Your Own AI Outputs

Take a real AI-generated summary you've used recently â€” a research synthesis, a competitive analysis, or a meeting summary â€” and run it through the verifier.

```bash
# Copy your real source docs into sources/
# Copy the AI analysis into analyses/
python verify.py
cat reports/*.md
```

**Success Checklist:**
- [ ] Real analysis processed successfully
- [ ] At least one claim flagged that you would have missed manually
- [ ] Verification report is clear enough to share with your team

#### Step 4: Add Batch Mode and Summary Stats

Enhance the tool to process multiple analyses and give you aggregate hallucination rates.

In Claude Code, prompt:

"Add a `--batch` flag to `verify.py` that processes all files in `analyses/` and appends a summary section at the end showing: total claims checked, % verified, % paraphrased, % unverifiable, % fabricated, and the top 3 most egregious fabrications."

```bash
python verify.py --batch
```

**Success Checklist:**
- [ ] Batch mode processes multiple analyses
- [ ] Summary stats are accurate
- [ ] You know the hallucination rate of your AI outputs

### ðŸŽ¯ Success Criteria

- [ ] Tool correctly distinguishes verified quotes from fabricated ones
- [ ] Catches at least one hallucination you would have missed
- [ ] Report format is clear enough to share as an audit trail
- [ ] You trust this tool enough to run it before any AI-assisted presentation

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--strict` mode that also checks numerical claims and dates
- Integrate with Google Docs API to verify quotes in shared documents
- Build a pre-commit hook that verifies AI outputs before they're added to the repo
- Add a Slack bot that auto-verifies any AI summary posted in your #research channel

*Inspired by: [@lennysan on X â€” AI data analysis produces made-up quotes and invented evidence with total confidence](https://x.com/lennysan)*

---

## Mission 5: Build a Context Window Optimizer for Long AI Sessions

**â±ï¸ Time:** 60-75 minutes
**ðŸ“Š Difficulty:** Advanced
**ðŸ› ï¸ Tools:** Claude Code, Python, Claude API

### ðŸ’¡ What You're Building

A wrapper around the Claude API that monitors token usage in real-time during long working sessions, detects when context quality is degrading, and automatically summarizes older messages to keep the AI sharp â€” solving the "context rot" problem where AI gets worse the longer you talk to it.

**You'll have:**
- A token counter that tracks context window usage as conversations grow
- An intelligent summarization system that condenses older messages when limits approach
- A context quality monitor that detects degradation patterns (repetition, vagueness, contradictions)
- A CLI dashboard showing context health metrics and when optimization kicks in

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- Experience with the Claude API (messages format, token counting)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project

Create the project and install dependencies.

```bash
mkdir -p ~/pm-tools/context-optimizer
cd ~/pm-tools/context-optimizer
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic tiktoken rich
touch optimizer.py conversation.py monitor.py
echo "Ready"
```

**Success Checklist:**
- [ ] Project created with all files
- [ ] `tiktoken` installed for token counting
- [ ] `rich` installed for CLI dashboard

#### Step 2: Ask Claude Code to Build the Token Tracker and Conversation Manager

In Claude Code, prompt:

"Write `conversation.py` â€” a ConversationManager class that: (1) stores messages in a list with role/content/token_count, (2) has an `add_message(role, content)` method that counts tokens using tiktoken (cl100k_base encoding) and appends, (3) has a `get_total_tokens()` method, (4) has a `get_context_window_usage()` that returns a percentage based on a 200k context window, (5) has a `summarize_old_messages(keep_recent=5)` method that takes all messages except the most recent N, sends them to Claude API with the prompt 'Summarize this conversation history in a concise paragraph, preserving all key decisions, facts, and action items', and replaces the old messages with a single summary message. (6) has a `get_messages_for_api()` that returns the message list in Claude API format."

```bash
python -c "from conversation import ConversationManager; c = ConversationManager(); c.add_message('user', 'Hello world'); print(f'Tokens: {c.get_total_tokens()}, Usage: {c.get_context_window_usage():.1%}')"
```

**Success Checklist:**
- [ ] Token counting works for any message
- [ ] Context window percentage is calculated correctly
- [ ] ConversationManager stores and retrieves messages

#### Step 3: Build the Quality Monitor

In Claude Code, prompt:

"Write `monitor.py` â€” a ContextQualityMonitor class that: (1) takes the last N assistant responses, (2) detects degradation signals: response getting shorter over time, increasing use of hedge words ('perhaps', 'maybe', 'it depends'), repetition of earlier phrases, or contradicting earlier statements, (3) returns a quality score 0-100 and a list of detected issues, (4) has a `should_optimize()` method that returns True when quality drops below 70 or token usage exceeds 60%."

```bash
python -c "
from monitor import ContextQualityMonitor
m = ContextQualityMonitor()
score, issues = m.analyze(['This is a detailed response about product strategy.', 'Maybe it depends on various factors perhaps.', 'It depends, perhaps maybe we should consider...'])
print(f'Quality: {score}/100, Issues: {issues}')
print(f'Should optimize: {m.should_optimize(score, token_usage_pct=0.3)}')
"
```

**Success Checklist:**
- [ ] Quality scoring detects hedge words and repetition
- [ ] Score drops when response quality degrades
- [ ] `should_optimize()` triggers at the right thresholds

#### Step 4: Build the Interactive Optimizer CLI

In Claude Code, prompt:

"Write `optimizer.py` â€” a CLI tool using `rich` that: (1) starts an interactive conversation with Claude, (2) shows a live status bar with: message count, total tokens, context usage %, quality score, (3) after each exchange, checks `should_optimize()`, (4) when triggered, shows '[Optimizing context...]', runs `summarize_old_messages()`, and shows the before/after token counts, (5) uses argparse with `--max-tokens` (default 200000) and `--quality-threshold` (default 70) flags."

```bash
python optimizer.py
# Have a long conversation â€” watch the token counter grow
# When it triggers optimization, see context get compressed
```

**Success Checklist:**
- [ ] Interactive chat works with live token counter
- [ ] Quality score updates after each response
- [ ] Optimization triggers automatically and compresses context
- [ ] Conversation quality stays high even after 20+ exchanges

### ðŸŽ¯ Success Criteria

- [ ] Token tracking is accurate within 5% of actual API usage
- [ ] Context summarization preserves key decisions and facts
- [ ] Quality monitor detects real degradation patterns
- [ ] A 30-message conversation stays sharp with auto-optimization enabled

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add conversation save/load to pick up long sessions across days
- Implement "memory tiers" â€” recent (full), medium (summarized), old (keywords only)
- Build an MCP server version so Claude Code itself uses the optimizer
- Add a `--replay` mode that loads a conversation log and shows where quality dropped

*Inspired by: [@ttorres on X â€” AI gets worse the longer you talk to it, context rot degrades performance as conversations grow](https://x.com/ttorres)*

---

## Mission 6: Build a Strategy Doc Validator That Detects Fluff

**â±ï¸ Time:** 45-60 minutes
**ðŸ“Š Difficulty:** Intermediate
**ðŸ› ï¸ Tools:** Claude Code, Python, Claude API

### ðŸ’¡ What You're Building

A tool that reads your product strategy document and tells you whether it contains actual strategic choices â€” trade-offs, constraints, specific bets â€” or if it's just dressed-up mission and vision statements. Because most strategy docs are aspirational fluff, not real strategy.

**You'll have:**
- A document parser that identifies mission, vision, and strategy sections
- A strategic choice detector that flags concrete trade-offs and specific bets
- A fluff analyzer that measures the ratio of aspirational statements vs. actionable strategy
- A scored report with section-by-section recommendations for making the strategy real

### âœ… Prerequisites

- Python 3.8+ installed
- ANTHROPIC_API_KEY set
- A product strategy document (or any strategy doc â€” even a blog post describing a company strategy)

### ðŸš€ Step-by-Step Instructions

**Execute these steps through Claude Code:**

#### Step 1: Set Up the Project and Create a Test Strategy Doc

Create the project and write a deliberately flawed strategy doc to test against.

```bash
mkdir -p ~/pm-tools/strategy-validator/{docs,reports}
cd ~/pm-tools/strategy-validator
python3 -m venv .venv && source .venv/bin/activate
pip install anthropic

cat > docs/sample-strategy.md << 'EOF'
# Product Strategy 2026

## Our Mission
To empower teams everywhere with world-class collaboration tools that drive innovation and delight users.

## Our Vision
A world where every team can work seamlessly, no matter where they are.

## Strategy
We will focus on delivering an exceptional user experience across all platforms. By leveraging AI and modern technology, we will create intuitive solutions that meet the evolving needs of our customers. We are committed to continuous improvement and customer satisfaction.

## Key Priorities
1. Improve the user experience
2. Expand into new markets
3. Leverage AI capabilities
4. Build a world-class team

## Success Metrics
- Increase NPS
- Grow revenue
- Improve retention
EOF

cat > docs/good-strategy.md << 'EOF'
# Product Strategy 2026

## Strategic Context
We are the #3 player in project management tools. Competitors 1 and 2 own enterprise. We cannot win enterprise deals against them â€” they have 10x our sales team and SOC2 Type II.

## Our Bet
We are betting that the 2-15 person team segment is underserved. These teams need the power of enterprise tools but refuse to pay enterprise prices or deal with enterprise setup. We will win this segment by being 10x faster to set up than any competitor.

## What We Are NOT Doing
- Enterprise features (SSO, SCIM, audit logs) â€” not this year
- Mobile app â€” our users work on laptops, mobile usage is <3%
- Integrations marketplace â€” we will support only Slack, GitHub, and Linear

## Key Trade-offs
- Speed over features: we ship fewer things but each one works in under 2 clicks
- Opinionated over flexible: we choose the default workflow, no custom fields in v1
- Self-serve over sales-assisted: $0 spent on outbound sales this year

## How We'll Know It's Working
- 50% of new signups complete setup in <5 minutes (currently 22%)
- 40% weekly active rate at day 30 (currently 28%)
- $2M ARR from teams of 2-15 by Q4 (currently $400K)
EOF

echo "Two strategy docs ready â€” one fluff, one real"
```

**Success Checklist:**
- [ ] Both strategy docs created
- [ ] You can see the difference â€” one is generic fluff, the other has real choices

#### Step 2: Ask Claude Code to Build the Validator

In Claude Code, prompt:

"Write a Python script `validate.py` that: (1) reads a strategy doc from `docs/`, (2) sends it to Claude API with this analysis prompt: 'Analyze this product strategy document. For each section, identify whether it contains: (a) STRATEGIC CHOICES â€” specific trade-offs, things explicitly NOT being done, concrete bets with downside risk, (b) ASPIRATIONAL FLUFF â€” generic statements any company could say, no trade-offs, no specifics, no measurable targets, (c) MEASURABLE COMMITMENTS â€” specific numbers, timelines, or thresholds. Score each section 0-100 where 100 = pure strategic choices and 0 = pure fluff. Give an overall Strategy Reality Score. For any section scoring below 50, provide a specific rewrite suggestion that adds a real strategic choice.' (3) Saves the report to `reports/validation-{filename}-{timestamp}.md`."

```bash
python validate.py docs/sample-strategy.md
cat reports/*sample*
echo "---"
python validate.py docs/good-strategy.md
cat reports/*good*
```

**Success Checklist:**
- [ ] Fluff doc scores low (below 30)
- [ ] Good strategy doc scores high (above 70)
- [ ] Each section gets individual scores
- [ ] Rewrite suggestions for low-scoring sections are specific, not generic

#### Step 3: Validate Your Real Strategy Doc

Drop your actual product strategy into the `docs/` folder and run the validator.

```bash
# Copy your real strategy doc into docs/
python validate.py docs/your-real-strategy.md
cat reports/*your-real*
```

**Success Checklist:**
- [ ] Your real doc processed successfully
- [ ] At least one section scored lower than you expected
- [ ] Rewrite suggestions give you something actionable

#### Step 4: Add Comparison Mode

In Claude Code, prompt:

"Add a `--compare` flag to `validate.py` that takes two strategy docs and generates a side-by-side analysis: which doc has more real strategic choices, where they overlap (both saying the same generic things), and a recommendation for which sections from each to keep."

```bash
python validate.py --compare docs/sample-strategy.md docs/good-strategy.md
```

**Success Checklist:**
- [ ] Side-by-side comparison generated
- [ ] Overlap detection identifies shared generic phrases
- [ ] Recommendation section is actionable

### ðŸŽ¯ Success Criteria

- [ ] Tool correctly distinguishes fluff strategy docs from real ones
- [ ] Strategy Reality Score correlates with your gut feeling about doc quality
- [ ] Rewrite suggestions make weak sections genuinely stronger
- [ ] You'd run this before your next strategy review meeting

### ðŸ° Next Steps (Optional)

Once you've completed the basics, try:

- Add a `--watch` mode that re-validates whenever the doc changes in your editor
- Build a Notion integration that validates strategy pages directly
- Create a "strategy anti-patterns" library that the tool checks against
- Add historical tracking to see if your strategy docs are getting more specific over time

*Inspired by: [@shreyas on X â€” most product strategy docs aren't real strategies because PMs start with mission/vision instead of strategic choices](https://x.com/shreyas)*

---
